"use strict";(function(){const t={cache:!0};t.doc={id:"id",field:["title","content"],store:["title","href","section"]};const e=FlexSearch.create("balance",t);window.bookSearchIndex=e,e.add({id:0,href:"/study-kubernetes/docs/advanced/eco/docker/image/alpine/",title:"alpine",section:"镜像",content:` alpine # 安装 telnet # apk add busybox-extras
`}),e.add({id:1,href:"/study-kubernetes/docs/basic/arch/component/api-server/",title:"api-server",section:"组件",content:" api-server 基础 # "}),e.add({id:2,href:"/study-kubernetes/docs/advanced/eco/docker/quick/",title:"Docker 快速上手",section:"Docker",content:" Docker 快速上手 # 安装 # MacOS 下载: https://download.docker.com/mac/stable/Docker.dmg Linux Windows 登陆阿里云镜像仓库 # docker login --username=657336244@qq.com registry.cn-shanghai.aliyuncs.com 密码：阿里云密码 拉取 k8s.gcr.io 镜像：\ndocker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.20.8 docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.20.8 k8s.gcr.io/kube-apiserver:v1.20.8 docker rmi -f registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.20.8 # 遇到 aliyun 没有的，可以直接在 https://hub.docker.com/ 搜索 docker pull bitnami/kube-state-metrics:2.1.1 cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF yum install -y kubelet-1.20.8 kubeadm-1.20.8 kubectl-1.20.8 systemctl enable kubelet &amp;&amp; systemctl start kubelet url=registry.cn-hangzhou.aliyuncs.com/google_containers version=v1.20.8 images=(`kubeadm config images list --kubernetes-version=$version|awk -F &#39;/&#39; &#39;{print $2}&#39;`) for imagename in ${images[@]} ; do docker pull $url/$imagename docker tag $url/$imagename k8s.gcr.io/$imagename docker rmi -f $url/$imagename done "}),e.add({id:3,href:"/study-kubernetes/docs/basic/arch/network/port/",title:"K8s 端口",section:"K8s 网络",content:` K8s 端口 # service 的端口 port：负责处理对内的通信， 访问方式：clusterIP:port 或者 externalIP:port externalIP 不归 kubernetes 管，cluster administrator 自己负责 nodePort：在 node 上，负责对外通信 访问方式：NodeIP:NodePort pod 的端口 targetPort：在 pod 上 从 port 和 nodePort 上来的流量，经过 kube-proxy 流入到后端 pod 的 targetPort 上，最后进入容器 容器的端口 containerPort：在容器上，用于被 pod 绑定 targetPort 和 containerPort 是一致的 是可选的，仅仅是提示信息 容器中任何监听 0.0.0.0 的端口，都会暴露出来 无法被更新 主机的端口 hostPort：容器暴露的端口映射到的主机端口 尽量不要为 Pod 指定 hostPort 将 Pod 绑定到 hostPort 时，它会限制 Pod 可以调度的位置数，因为每个 &lt;hostIP, hostPort, protocol&gt; 组合必须是唯一的 如果您没有明确指定 hostIP 和 protocol，Kubernetes 将使用 0.0.0.0 作为默认 hostIP 和 TCP 作为默认 protocol pod 模板指定端口 # 作用类似于 docker -p 选项
containerPort: 容器需要暴露的端口 hostPort: 容器暴露的端口映射到的主机端口 service 指定端口 # service (ClusterIP) # port: service 中 clusterIP 对应的端口 targetPort: clusterIP 作为负载均衡，后端目标实例 (容器) 的端口 service (NodePort) # nodePort: cluster ip 只能集群内部访问 范围是 30000-32767，这个值在 API server 的配置文件中，用 --service-node-port-range 定义 源与目标需要满足两个条件: kube-proxy 正常运行 跨主机容器网络通信正常 nodePort 会在每个 kubelet 节点的宿主机开启一个端口，用于应用集群外部访问 nodePort 到 clusterIP 的映射，是 kube-proxy 实现的。 参考 # 配置最佳实践 从外部访问 Kubernetes 中的 Pod 关于在 kubenretes 中暴露 Pod 及服务的 5 种方式 `}),e.add({id:4,href:"/study-kubernetes/docs/basic/concept/resource/pod/",title:"Pod",section:"API 对象",content:" Pod # 操作 # 删除被驱逐的 pod\nkubectl get pods | grep Evicted | awk &#39;{print $1}&#39; | xargs kubectl delete pod Infra 容器 # 使用镜像：k8s.gcr.io/pause\n这个镜像是一个用汇编语言编写的、永远处于 “暂停” 状态的容器，解压后的大小也只有 100~200 KB 左右。\nInit 容器 # 以 Init: 开始的 Pod 状态概括表示 Init 容器的执行状态。\n下表展示了在调试 Init 容器时可能见到的状态值。\n状态 含义 Init:N/M Pod 中有 M 个 Init 容器，其中 M 已经完成 Init:Error Init 容器执行错误 Init:CrashLoopBackOff Init 容器已经失败多次 Pending Pod 还没有开始执行 Init 容器 PodInitializing or Running Pod 已经完成执行 Init 容器 如果一个 Pod 停滞在 Pending 状态，表示 Pod 没有被调度到节点上。通常这是因为 某种类型的资源不足导致无法调度。 查看上面的 kubectl describe &hellip; 命令的输出，其中应该显示了为什么没被调度的原因。\n常见原因:\n资源不足 使用了 hostPort 参考：\n应用故障排查 PodStatus # type PodStatus struct { // The phase of a Pod is a simple, high-level summary of where the Pod is in its lifecycle. // The conditions array, the reason and message fields, and the individual container status // arrays contain more detail about the pod&#39;s status. // There are five possible phase values: // // Pending: The pod has been accepted by the Kubernetes system, but one or more of the // container images has not been created. This includes time before being scheduled as // well as time spent downloading images over the network, which could take a while. // Running: The pod has been bound to a node, and all of the containers have been created. // At least one container is still running, or is in the process of starting or restarting. // Succeeded: All containers in the pod have terminated in success, and will not be restarted. // Failed: All containers in the pod have terminated, and at least one container has // terminated in failure. The container either exited with non-zero status or was terminated // by the system. // Unknown: For some reason the state of the pod could not be obtained, typically due to an // error in communicating with the host of the pod. // // More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#pod-phase // +optional Phase PodPhase `json:&#34;phase,omitempty&#34; protobuf:&#34;bytes,1,opt,name=phase,casttype=PodPhase&#34;` // Current service state of pod. // More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#pod-conditions // +optional // +patchMergeKey=type // +patchStrategy=merge Conditions []PodCondition `json:&#34;conditions,omitempty&#34; patchStrategy:&#34;merge&#34; patchMergeKey:&#34;type&#34; protobuf:&#34;bytes,2,rep,name=conditions&#34;` // A human readable message indicating details about why the pod is in this condition. // +optional Message string `json:&#34;message,omitempty&#34; protobuf:&#34;bytes,3,opt,name=message&#34;` // A brief CamelCase message indicating details about why the pod is in this state. // e.g. &#39;Evicted&#39; // +optional Reason string `json:&#34;reason,omitempty&#34; protobuf:&#34;bytes,4,opt,name=reason&#34;` // nominatedNodeName is set only when this pod preempts other pods on the node, but it cannot be // scheduled right away as preemption victims receive their graceful termination periods. // This field does not guarantee that the pod will be scheduled on this node. Scheduler may decide // to place the pod elsewhere if other nodes become available sooner. Scheduler may also decide to // give the resources on this node to a higher priority pod that is created after preemption. // As a result, this field may be different than PodSpec.nodeName when the pod is // scheduled. // +optional NominatedNodeName string `json:&#34;nominatedNodeName,omitempty&#34; protobuf:&#34;bytes,11,opt,name=nominatedNodeName&#34;` // IP address of the host to which the pod is assigned. Empty if not yet scheduled. // +optional HostIP string `json:&#34;hostIP,omitempty&#34; protobuf:&#34;bytes,5,opt,name=hostIP&#34;` // IP address allocated to the pod. Routable at least within the cluster. // Empty if not yet allocated. // +optional PodIP string `json:&#34;podIP,omitempty&#34; protobuf:&#34;bytes,6,opt,name=podIP&#34;` // RFC 3339 date and time at which the object was acknowledged by the Kubelet. // This is before the Kubelet pulled the container image(s) for the pod. // +optional StartTime *metav1.Time `json:&#34;startTime,omitempty&#34; protobuf:&#34;bytes,7,opt,name=startTime&#34;` // The list has one entry per init container in the manifest. The most recent successful // init container will have ready = true, the most recently started container will have // startTime set. // More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#pod-and-container-status InitContainerStatuses []ContainerStatus `json:&#34;initContainerStatuses,omitempty&#34; protobuf:&#34;bytes,10,rep,name=initContainerStatuses&#34;` // The list has one entry per container in the manifest. Each entry is currently the output // of `docker inspect`. // More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#pod-and-container-status // +optional ContainerStatuses []ContainerStatus `json:&#34;containerStatuses,omitempty&#34; protobuf:&#34;bytes,8,rep,name=containerStatuses&#34;` // The Quality of Service (QOS) classification assigned to the pod based on resource requirements // See PodQOSClass type for available QOS classes // More info: https://git.k8s.io/community/contributors/design-proposals/node/resource-qos.md // +optional QOSClass PodQOSClass `json:&#34;qosClass,omitempty&#34; protobuf:&#34;bytes,9,rep,name=qosClass&#34;` } Pod phase（运行阶段） # Pod 的 status 定义在 PodStatus 对象中，其中有一个 Phase（运行阶段） 字段。\n下面是 phase 可能的值：\n状态 涵义 备注 Pending Pod 被系统接受，至少有一个容器未被创建 Running Pod 已绑定 Node，所有容器已创建，至少一个容器在运行（或处于正在启动/重启） Succeeded Pod 中所有容器被成功终止，不再重启 Failed Pod 中所有容器被终止，至少有一个容器是因为失败终止（非 0 退出或被系统终止） Unknown 无法获取 Pod 状态（与 Pod 所在主机通信失败） PodCondition # Pod 有一个 PodStatus 对象，其中包含一个 PodCondition 数组。\nPodCondition 数组的每个元素都有一个 type 字段和一个 status 字段。\ntype 字段是字符串，可能的值有 PodScheduled Ready Initialized Unschedulable status 字段是一个字符串，可能的值有 True False Unknown Terminating # ContainerCreating # 资源限制 # request # requests 用于 schedule 阶段，在调度 pod 保证所有 pod 的 requests 总和小于 node 能提供的计算能力 requests.cpu 被转成 docker 的 --cpu-shares 参数，与 cgroup cpu.shares 功能相同 设置容器的 cpu 的相对权重 该参数在 CPU 资源不足时生效，根据容器 requests.cpu 的比例来分配 cpu 资源 CPU 资源充足时，requests.cpu 不会限制 container 占用的最大值，container 可以独占 CPU requests.memory 没有对应的 docker 参数，作为 k8s 调度依据 使用 requests 来设置各容器需要的最小资源 limit # limits 限制运行时容器占用的资源 limits.cpu 会被转换成 docker 的&ndash;cpu-quota 参数。与 cgroup cpu.cfs_quota_us 功能相同 限制容器的最大 CPU 使用率。 cpu.cfs_quota_us 参数与 cpu.cfs_period_us 结合使用，后者设置时间周期 k8s 将 docker 的&ndash;cpu-period 参数设置 100 毫秒。对应着 cgroup 的 cpu.cfs_period_us limits.cpu 的单位使用 m，千分之一核 limits.memory 会被转换成 docker 的&ndash;memory 参数。用来限制容器使用的最大内存 当容器申请内存超过 limits 时会被终止 问答 # 为什么要有 pod # 参考 # "}),e.add({id:5,href:"/study-kubernetes/docs/basic/concept/resource/replicaset/",title:"replicaset",section:"API 对象",content:" replicaset # 问答 # replicaset vs deployment？ # ReplicaSet 是下一代 ReplicationController， 支持新的基于集合的标签选择算符。 ReplicaSet 并不直接支持滚动更新，参考 ReplicationController 的设计目的是通过逐个替换 Pod 以方便滚动更新服务，参考 "}),e.add({id:6,href:"/study-kubernetes/docs/basic/install/distro/",title:"发行版",section:"安装",content:` Kubernetes 发行版 # Kubeadm # kubernetes/kubeadm Kubeadm is a tool built to provide best-practice &ldquo;fast paths&rdquo; for creating Kubernetes clusters. It performs the actions necessary to get a minimum viable, secure cluster up and running in a user friendly way. Kubeadm&rsquo;s scope is limited to the local node filesystem and the Kubernetes API, and it is intended to be a composable building block of higher level tools.
用于快速搭建 kubernetes 集群，目前应该是比较方便和推荐的，简单易用 kubeadm 是 Kubernetes 1.4 开始新增的特性 kubeadm init 以及 kubeadm join 这两个命令可以快速创建 kubernetes 集群 参考：
https://www.katacoda.com Katacoda - Interactive Learning Platform for Software Engineers Getting Started With Kubeadm from katacoda MicroK8s # ubuntu/microk8s MicroK8s is a small, fast, single-package Kubernetes for developers, IoT and edge.
https://microk8s.io
Pros: Very easy to install, upgrade, remove Completely isolated from other tools in your machine Does not need a VM, all services run locally Cons: Only available for Snap supported Linux Distributions Relatively new, possible unstable Minikube can also run directly on Linux (vmdriver=none), so MicroK8s value proposition is diminished 参考：
Local Kubernetes for Linux – MiniKube vs MicroK8s MiniKube # kubernetes/minikube Run Kubernetes locally
https://minikube.sigs.k8s.io
安装：
MacOS 1. Install kubectl # 2. Install a Hypervisor # HyperKit VirtualBox VMware Fusion 3. Install Minikube # Homebrew # brew install minikube
curl # curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64 \\ &amp;&amp; chmod +x minikube sudo mv minikube /usr/local/bin Linux Windows 参考：https://kubernetes.io/docs/tasks/tools/install-minikube/
启动：
minikube start --registry-mirror=https://registry.docker-cn.com --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers
minikube start --cpus=2 --disk-size='10g' --image-mirror-country='cn' --image-repository='registry.cn-hangzhou.aliyuncs.com/google_containers'
minikube start --image-mirror-country='cn' --registry-mirror=https://registry.docker-cn.com kind # kubernetes-sigs/kind Kubernetes IN Docker - local clusters for testing Kubernetes
`}),e.add({id:7,href:"/study-kubernetes/docs/basic/install/",title:"安装",section:"第一部分 基础入门",content:` Kubernetes 安装 # Kubernetes 各组件配置文件 # kubelet # kubelet 是通过 systemctl 来管理的，因此配置文件在 /etc/systemd/system/ 或 /usr/lib/systemd/system/
# 修改配置文件 vi /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf # kubeadm init 生成的 # 使用 systemd vi /var/lib/kubelet/config.yaml # 配置 6443 vi /etc/kubernetes/kubelet.conf # 重启 systemctl daemon-reload &amp;&amp; systemctl restart kubelet # 查看 kubelet 错误日志 journalctl -xef | egrep &#34;: [F][0-9]&#34; -B 1 kubeadm vs kops vs kubespray # Kubespray 基于 Ansible 实现配置和编排 更灵活，适用多平台 Kops 自己实现配置和编排 适用某个固定平台 kubeadm 需要领域知识，类似 Kubernetes cluster operator 从 v2.3 开始，kubespray 底层使用 kubeadm 教程 # 和我一步步部署 kubernetes 集群 # opsnull/follow-me-install-kubernetes-cluster 本系列文档介绍使用二进制部署 kubernetes v1.16.6 集群的所有步骤（Hard-Way 模式）。
在部署的过程中，将详细列出各组件的启动参数，它们的含义和可能遇到的问题。
https://k8s-install.opsnull.com/
`}),e.add({id:8,href:"/study-kubernetes/docs/basic/install/config/",title:"配置",section:"安装",content:` Kubernetes 配置 # apiserver # 修改 Kubernetes apiserver 启动参数 # 如果您使用 kubeadm 安装 Kubernetes 集群，Kubernetes apiserver 通过 static pod 启动，
其 yaml 文件的位置在 /etc/kubernetes/manifests/kube-apiserver.yaml
Static Pod 的配置文件被修改后，立即生效。 Kubelet 会监听该文件的变化，当您修改了 /etc/kubenetes/manifest/kube-apiserver.yaml 文件之后，kubelet 将自动终止原有的 kube-apiserver-{nodename} 的 Pod，并自动创建一个使用了新配置参数的 Pod 作为替代。 如果您有多个 Kubernetes Master 节点，您需要在每一个 Master 节点上都修改该文件，并使各节点上的参数保持一致。 参考：
修改 Kubernetes apiserver 启动参数 `}),e.add({id:9,href:"/study-kubernetes/docs/basic/concept/resource/configmap/",title:"ConfigMap",section:"API 对象",content:` ConfigMap 基础 # ConfigMap 在运行时会将配置文件、命令行参数、环境变量、端口号以及其他配置工件绑定到 Pod 的容器和系统组件。借助 ConfigMap，您可以将配置与 Pod 和组件分开，这有助于保持工作负载的可移植性，使其配置更易于更改和管理，并防止将配置数据硬编码到 Pod 规范。
ConfigMap 可用于存储和共享非敏感、未加密的配置信息。要在集群中使用敏感信息，您必须使用 Secret。
创建 ConfigMap # 使用以下命令创建 ConfigMap：
kubectl create configmap [NAME] [DATA]
[DATA] 可以是： 包含一个或多个配置文件的目录的路径，使用 &ndash;from-file 标志指示 键值对，每个键值对都使用 &ndash;from-literal 标志指定 如需详细了解 kubectl create，请参阅参考文档。
您还可以通过在 YAML 清单文件中定义 ConfigMap 对象并使用 kubectl create -f [FILE] 部署对象来创建 ConfigMap。
使用 ConfigMap # apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: k8s.gcr.io/busybox command: [&#34;/bin/sh&#34;, &#34;-c&#34;, &#34;echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)&#34;] env: - name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: name: special-config key: SPECIAL_LEVEL - name: SPECIAL_TYPE_KEY valueFrom: configMapKeyRef: name: special-config key: SPECIAL_TYPE restartPolicy: Never `}),e.add({id:10,href:"/study-kubernetes/docs/basic/concept/resource/deployment/",title:"Deployment",section:"API 对象",content:` Deployment # 故障排查 # 下载 pdf 版本
参考：
A visual guide on troubleshooting Kubernetes deployments Kubernetes Deployment 故障排查常见方法 问答 # 为什么要有 Deployment # `}),e.add({id:11,href:"/study-kubernetes/docs/basic/arch/component/schedule/",title:"Schedule",section:"组件",content:" Schedule 基础 # "}),e.add({id:12,href:"/study-kubernetes/docs/basic/arch/component/controller-manager/",title:"Controller Manager",section:"组件",content:" Controller Manager 基础 # "}),e.add({id:13,href:"/study-kubernetes/docs/basic/concept/resource/crd/",title:"CRD",section:"API 对象",content:` CRD # 如何实现一个 CRD ？ # 关注 # GoogleCloudPlatform/metacontroller # Metacontroller is an add-on for Kubernetes that makes it easy to write and deploy custom controllers in the form of simple scripts.
This is not an officially supported Google product. Although this open-source project was started by GKE, the add-on works the same in any Kubernetes cluster.
`}),e.add({id:14,href:"/study-kubernetes/docs/design/",title:"第三部分 设计与实现",section:"Docs",content:` 如无特殊说明，源码版本为 1.18.2
v1.18 Release Notes
`}),e.add({id:15,href:"/study-kubernetes/docs/basic/concept/resource/",title:"API 对象",section:"1.2 概念",content:` API 对象 # NAME SHORTNAMES APIGROUP NAMESPACED KIND bindings true Binding componentstatuses cs false ComponentStatus configmaps cm true ConfigMap endpoints ep true Endpoints events ev true Event limitranges limits true LimitRange namespaces ns false Namespace nodes no false Node persistentvolumeclaims pvc true PersistentVolumeClaim persistentvolumes pv false PersistentVolume pods po true Pod podtemplates true PodTemplate replicationcontrollers rc true ReplicationController resourcequotas quota true ResourceQuota secrets true Secret serviceaccounts sa true ServiceAccount services svc true Service initializerconfigurations admissionregistration.k8s.io false InitializerConfiguration mutatingwebhookconfigurations admissionregistration.k8s.io false MutatingWebhookConfiguration validatingwebhookconfigurations admissionregistration.k8s.io false ValidatingWebhookConfiguration customresourcedefinitions crd,crds apiextensions.k8s.io false CustomResourceDefinition apiservices apiregistration.k8s.io false APIService controllerrevisions apps true ControllerRevision daemonsets ds apps true DaemonSet deployments deploy apps true Deployment replicasets rs apps true ReplicaSet statefulsets sts apps true StatefulSet auditsinks auditregistration.k8s.io false AuditSink tokenreviews authentication.k8s.io false TokenReview localsubjectaccessreviews authorization.k8s.io true LocalSubjectAccessReview selfsubjectaccessreviews authorization.k8s.io false SelfSubjectAccessReview selfsubjectrulesreviews authorization.k8s.io false SelfSubjectRulesReview subjectaccessreviews authorization.k8s.io false SubjectAccessReview horizontalpodautoscalers hpa autoscaling true HorizontalPodAutoscaler cronjobs cj batch true CronJob jobs batch true Job certificatesigningrequests csr certificates.k8s.io false CertificateSigningRequest adapters config.istio.io true adapter attributemanifests config.istio.io true attributemanifest handlers config.istio.io true handler httpapispecbindings config.istio.io true HTTPAPISpecBinding httpapispecs config.istio.io true HTTPAPISpec instances config.istio.io true instance quotaspecbindings config.istio.io true QuotaSpecBinding quotaspecs config.istio.io true QuotaSpec rules config.istio.io true rule templates config.istio.io true template leases coordination.k8s.io true Lease events ev events.k8s.io true Event daemonsets ds extensions true DaemonSet deployments deploy extensions true Deployment ingresses ing extensions true Ingress networkpolicies netpol extensions true NetworkPolicy podsecuritypolicies psp extensions false PodSecurityPolicy replicasets rs extensions true ReplicaSet istiooperators iop install.istio.io true IstioOperator destinationrules dr networking.istio.io true DestinationRule envoyfilters networking.istio.io true EnvoyFilter gateways gw networking.istio.io true Gateway serviceentries se networking.istio.io true ServiceEntry sidecars networking.istio.io true Sidecar virtualservices vs networking.istio.io true VirtualService workloadentries we networking.istio.io true WorkloadEntry networkpolicies netpol networking.k8s.io true NetworkPolicy poddisruptionbudgets pdb policy true PodDisruptionBudget podsecuritypolicies psp policy false PodSecurityPolicy clusterrolebindings rbac.authorization.k8s.io false ClusterRoleBinding clusterroles rbac.authorization.k8s.io false ClusterRole rolebindings rbac.authorization.k8s.io true RoleBinding roles rbac.authorization.k8s.io true Role clusterrbacconfigs rbac.istio.io false ClusterRbacConfig rbacconfigs rbac.istio.io true RbacConfig servicerolebindings rbac.istio.io true ServiceRoleBinding serviceroles rbac.istio.io true ServiceRole priorityclasses pc scheduling.k8s.io false PriorityClass authorizationpolicies security.istio.io true AuthorizationPolicy peerauthentications pa security.istio.io true PeerAuthentication requestauthentications ra security.istio.io true RequestAuthentication podpresets settings.k8s.io true PodPreset storageclasses sc storage.k8s.io false StorageClass volumeattachments storage.k8s.io false VolumeAttachment 声明式 API # 提交一个定义好的 API 对象来 “声明” 启动一个对应的控制器进行调谐 / 编排 控制器 # 一个 Kubernetes 的控制器，实际上就是一个死循环：
不断地获取 “实际状态”， 然后与 “期望状态” 作对比， 并以此为依据决定下一步的操作 workload # Kubernetes divides workloads into different types. The most popular types supported by Kubernetes are:
Deployments StatefulSets DaemonSets Jobs CronJobs 参考：
Kubernetes Workloads and Pods API Resource # Deployment # metadata name spec replicas selector matchLabels app version template metadata annotations labels app version spec serviceAccountName containers name image imagePullPolicy ports name containerPort Service # metadata name labels app spec ports name port targetPort selector app CustomResourceDefinition # metadata name spec group version names kind plural scope `}),e.add({id:16,href:"/study-kubernetes/docs/basic/arch/component/kube-proxy/",title:"Kube-proxy",section:"组件",content:" Kube-proxy # "}),e.add({id:17,href:"/study-kubernetes/docs/basic/arch/component/kubelet/",title:"Kubelet",section:"组件",content:` Kubelet # kubelet 默认最多运行 110 个 pods
$ kubectl describe node xxxx | grep -A 7 &#39;Capacity&#39; Capacity: cpu: 4 ephemeral-storage: 41152716Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 16431024Ki pods: 110 kubelet 的 --max-pods 选项可以指定运行的最大 Pod 数目 因为 flanneld 配置的本节点 Pod 网段是 /24，所以一个 Node 最多运行 254 个 Pod（flannel、docker0 占用 2 个），--max-pods 不能超过该值； Kubelet 创建容器进程 # CRI: Container Runtime Interface CNI: Container Network Interface CSI: Container Storage Interface OCI: Open Container Initiative Docker # kubelet -&gt; dockershim -&gt; docker daemon -&gt; containerd -&gt; containerd-shim -&gt; runc -&gt; container
参考：
白话 Kubernetes Runtime containerd, containerd-shim 和 runc 的依存关系 containerd # containerd/containerd kubelet -&gt; cri plugin -&gt; containerd -&gt; containerd-shim -&gt; runc -&gt; container
Kubernetes v1.20 弃用 docker-shim # `}),e.add({id:18,href:"/study-kubernetes/docs/basic/arch/network/dns/",title:"K8s DNS",section:"K8s 网络",content:` K8s DNS # DNS 策略 # Default The Pod inherits the name resolution configuration from the node that the pods run on ClusterFirst Any DNS query that does not match the configured cluster domain suffix, such as “www.kubernetes.io”, is forwarded to the upstream nameserver inherited from the node. ClusterFirstWithHostNet For Pods running with hostNetwork, you should explicitly set its DNS policy “ClusterFirstWithHostNet” None It allows a Pod to ignore DNS settings from the Kubernetes environment. All DNS settings are supposed to be provided using the dnsConfig field in the Pod Spec. “Default” is not the default DNS policy.
If dnsPolicy is not explicitly specified, then “ClusterFirst” is used.
参考：
Pod&rsquo;s DNS Policy k8s 中域名是如何被解析的 # 在 k8s 中，一个 Pod 如果要访问相同 Namespace 下的 Service（比如 user-svc），那么只需要 curl user-svc。 如果 Pod 和 Service 不在同一域名下，那么就需要在 Service Name 之后添加上 Service 所在的 Namespace（比如 beta），curl user-svc.beta。 那么 k8s 是如何知道这些域名是内部域名并为他们做解析的呢？
无论是在 宿主机 或者是在 k8s 集群中，DNS 解析会依赖这个三个文件
/etc/host.conf /etc/hosts /etc/resolv.conf /etc/resolv.conf # resolv.conf 是 Pod 在 dnsPolicy: ClusterFirst 的情况下，k8s 为其自动生成的。 在该 Pod 内请求的所有的域名解析都需要经过 DNS Service 进行解析，不管是集群内部域名还是外部域名。
每行都会以一个关键字开头，然后跟配置参数。
在集群中主要使用到的关键词有 3 个
nameserver 定义 DNS 服务器的 IP 地址（Kube-DNS 的 Service IP） search 定义域名的搜索列表，当查询的域名中包含的 . 的数量少于 options.ndots 的值时，会依次匹配列表中的每个值 options 定义域名查找时的配置信息 nameserver、search 和 options 都是可以通过 dnsConfig 字段进行配置的，详细参考官方文档
例如
nameserver 10.250.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5 nameserver # nameserver 所对应的地址正是 DNS Service 的 Cluster IP（该值在启动 kubelet 的时候，通过 clusterDNS 指定）。
search 域 # search 域默认包含了 namespace.svc.cluster.local、svc.cluster.local 和 cluster.local 三种。
当我们在 Pod 中访问 a Service 时（ curl a ），会选择 nameserver 10.250.0.10 进行解析，然后依次带入 search 域进行 DNS 查找，直到找到为止。
$ curl a a.default.svc.cluster.local 显然因为 Pod 和 a Service 在同一 Namespace 下，所以第一次 lookup 就能找到。
如果 Pod 要访问不同 Namespace（例如： beta ）下的 Service b （ curl b.beta ），会经过两次 DNS 查找，分别是
$ curl b.beta b.beta.default.svc.cluster.local # Not Found b.beta.svc.cluster.local # Found 正是因为 search 的顺序性，所以访问同一 Namespace 下的 Service， curl a 是要比 curl a.default 的效率更高的，因为后者多经过了一次 DNS 解析。
$ curl a a.default.svc.cluster.local # Found $ curl a.default b.default.default.svc.cluster.local # Not Found b.default.svc.cluster.local # Found options # ndots # ndots:5，表示：
如果需要 lookup 的 Domain 中包含少于 5 个 . ，那么将会被当做非绝对域名， 如果需要查询的 Domain 中包含大于或等于 5 个 . ，那么就会被当做绝对域名。 如果是绝对域名则不会走 search 域，
如果是非绝对域名，就会按照 search 域中进行逐一匹配查询， 如果 search 走完了都没有找到，那么就会使用原域名进行查找。
优化外网域名解析 # 在真正解析 http://iftech.io 之前，经历了
iftech.io.default.svc.cluster.local -&gt; iftech.io.svc.cluster.local -&gt; iftech.io.cluster.local -&gt; iftech.io 这样也就意味着前 3 次 DNS 请求是浪费的，没有意义的。
直接使用绝对域名 # 这是最简单直接的优化方式，可以直接在要访问的域名后面加上 . 如：iftech.io. ，这样就可以避免走 search 域进行匹配。
配置 ndots # 比如配置 ndots:1，iftech.io. 就会使用原域名进行查找。
CoreDNS vs KubeDNS # 在 kube-dns 中，一个 pod 内使用了数个容器：kubedns、dnsmasq 和 sidecar。
kubedns 容器监视 Kubernetes API 并基于 Kubernetes DNS 规范提供 DNS 记录， dnsmasq 提供缓存和存根域支持， sidecar 提供指标和健康检查。 此设置会导致一些问题随着时间的推移而出现。首先，dnsmasq 中的安全漏洞导致过去需要发布 Kubernetes 安全补丁。 此外，由于 dnsmasq 处理存根域，但 kubedns 处理 External Services，因此你无法在外部服务中使用存根域，这非常限制该功能（参阅 dns＃131）。
在 CoreDNS 中，所有这些功能都在一个容器中完成 —— 该容器运行用 Go 编写的进程。 启用的不同插件来复制（并增强）kube-dns 中的功能。
为什么 pod 是 coredns，service 是 kube-dns？ # 其实是 CoreDNS
CoreDNS is default from K8S 1.11. For previous installations it&rsquo;s kube-dns.
看 image，其他都是 metadata，不重要
$ k describe pod coredns-6967fb4995-76trs -n kube-system | grep -i &#34;image&#34; Image: registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.3.1 问题详情
$ k cluster-info Kubernetes master is running at https://192.168.99.102:8443 KubeDNS is running at https://192.168.99.102:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy $ k get po -n kube-system NAME READY STATUS RESTARTS AGE coredns-6967fb4995-76trs 1/1 Running 6 32d coredns-6967fb4995-grnzj 1/1 Running 6 32d etcd-minikube 1/1 Running 3 32d kube-addon-manager-minikube 1/1 Running 3 32d kube-apiserver-minikube 1/1 Running 3 32d kube-controller-manager-minikube 1/1 Running 3 32d kube-proxy-z765q 1/1 Running 3 32d kube-scheduler-minikube 1/1 Running 3 32d storage-provisioner 1/1 Running 5 32d $ k get svc -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-dns ClusterIP 10.96.0.10 &lt;none&gt; 53/UDP,53/TCP,9153/TCP 32d $ k describe svc/kube-dns -n kube-system Name: kube-dns Namespace: kube-system Labels: k8s-app=kube-dns kubernetes.io/cluster-service=true kubernetes.io/name=KubeDNS Annotations: prometheus.io/port: 9153 prometheus.io/scrape: true Selector: k8s-app=kube-dns Type: ClusterIP IP: 10.96.0.10 Port: dns 53/UDP TargetPort: 53/UDP Endpoints: 172.17.0.2:53,172.17.0.3:53 Port: dns-tcp 53/TCP TargetPort: 53/TCP Endpoints: 172.17.0.2:53,172.17.0.3:53 Port: metrics 9153/TCP TargetPort: 9153/TCP Endpoints: 172.17.0.2:9153,172.17.0.3:9153 Session Affinity: None Events: &lt;none&gt; 参考：
Kubernetes: Kube-DNS vs CoreDNS `}),e.add({id:19,href:"/study-kubernetes/docs/basic/arch/network/",title:"K8s 网络",section:"1.3 架构",content:" K8s 网络 # "}),e.add({id:20,href:"/study-kubernetes/docs/basic/arch/network/troubleshoot/",title:"故障排查",section:"K8s 网络",content:" Kubernetes 网络故障排查 # "}),e.add({id:21,href:"/study-kubernetes/docs/basic/quick/",title:"1.1 快速上手",section:"第一部分 基础入门",content:` Kubernetes 快速上手 # ketacoda # Kubernetes basics (by lizrice) # In this scenario you&rsquo;ll run your Go application in a Kubernetes cluster.
`}),e.add({id:22,href:"/study-kubernetes/docs/design/component/",title:"3.1 Kubernetes 组件",section:"第三部分 设计与实现",content:" Kubernetes 组件 # "}),e.add({id:23,href:"/study-kubernetes/docs/appendix/tutorial/",title:"4.1 教程",section:"第四部分 附录",content:` Kubernetes 教程 # 基础 # The Kubernetes Learning Resources List # collabnix/dockerlabs # Docker - Beginners | Intermediate | Advanced https://dockerlabs.collabnix.com
To get started with Kubernetes, follow the below steps:
Open https://labs.play-with-kubernetes.com on your browser Click on Add Instances to setup first k8s node cluster
kelseyhightower/kubernetes-the-hard-way # Bootstrap Kubernetes the hard way on Google Cloud Platform. No scripts.
feiskyer/kubernetes-handbook # Kubernetes Handbook （Kubernetes 指南） https://kubernetes.feisky.xyz
play-with-docker/play-with-docker # Play With Docker gives you the experience of having a free Alpine Linux Virtual Machine in the cloud where you can build and run Docker containers and even create clusters with Docker features like Swarm Mode.
Under the hood DIND or Docker-in-Docker is used to give the effect of multiple VMs/PCs.
A live version is available at: http://play-with-docker.com/
进阶 # jamiehannaford/what-happens-when-k8s # What happens when I type kubectl run?
daniel-hutao/k8s-source-code-analysis # 《k8s-1.13 版本源码分析》 https://hutao.tech/k8s-source-code-analysis/
`}),e.add({id:24,href:"/study-kubernetes/docs/basic/arch/network/dns/coredns/",title:"CoreDNS",section:"K8s DNS",content:" CoreDNS # coredns/coredns "}),e.add({id:25,href:"/study-kubernetes/docs/advanced/eco/docker/",title:"Docker",section:"2.8 生态",content:" Docker 基础 # docker 资源 # docker container ls：默认只列出正在运行的容器，-a 选项会列出包括停止的所有容器。 docker image ls：列出镜像信息，-a 选项会列出 intermediate 镜像 (就是其它镜像依赖的层)。 docker volume ls：列出数据卷。 docker network ls：列出 network。 docker info：显示系统级别的信息，比如容器和镜像的数量等。 清理 docker 资源 # 删除那些已停止的容器、dangling 镜像、未被容器引用的 network 和构建过程中的 cache\ndocker system prune # 默认不会删除那些未被任何容器引用的数据卷，需要使用 --volumes 进行删除 docker system prune --volumes # 直接删除，没有确认过程 docker system prune --all --force --volumes docker container prune # 删除所有退出状态的容器 docker volume prune # 删除未被使用的数据卷 docker image prune # 删除 dangling 或所有未被使用的镜像 docker container rm $(docker container ls -a -q) # 删除容器 docker image rm $(docker image ls -a -q) # 删除镜像 docker volume rm $(docker volume ls -q) # 删除数据卷 docker network rm $(docker network ls -q) # 删除 network # 列出所有容器 docker container ls -a -q # 停止所有容器 docker container stop $(docker container ls -a -q) # 删除所有资源 docker container stop $(docker container ls -a -q) &amp;&amp; docker system prune --all --force --volumns 参考：\n如何快速清理 docker 资源 "}),e.add({id:26,href:"/study-kubernetes/docs/advanced/feature/operator/kubebuilder/",title:"kubebuilder",section:"Operator",content:` kubebuilder # kubernetes-sigs/kubebuilder # Kubebuilder - SDK for building Kubernetes APIs using CRDs http://book.kubebuilder.io/
`}),e.add({id:27,href:"/study-kubernetes/docs/advanced/tool/kubectl/",title:"Kubectl",section:"2.2 工具",content:` Kubectl # 集群信息查询 # kubectl cluster-info node # kubectl get nodes kubectl describe node 常用操作 # 设置 # 设置 role # # 添加 kubectl label node ime-rd5-edge0-node2-kunlun kubernetes.io/role=master kubectl label node ime-rd5-edge0-node2-kunlun kubernetes.io/role=worker # 删除 kubectl label node ime-rd5-edge0-node2-kunlun kubernetes.io/role=worker # 重置 role kubectl label --overwrite nodes &lt;your_node&gt; kubernetes.io/role=&lt;your_new_label&gt; 获取 service ip, port # kubectl get service/servicename -o jsonpath=&#39;{.spec.clusterIP}:{.spec.ports[*].port}&#39; kubectl get 结果排序 # --sort-by= ##### pod # name kubectl --sort-by=.metadata.name get pod # status kubectl --sort-by=.status.phase get pod # restarts kubectl --sort-by=&#39;.status.containerStatuses[0].restartCount&#39; get pod # age kubectl --sort-by=.status.startTime get pod # ip kubectl --sort-by=.status.podIP get pod # node kubectl --sort-by=.spec.nodeName get pod ##### deployment # name kubectl --sort-by=.metadata.name get deployment # age kubectl --sort-by=.metadata.creationTimestamp get deployment # uptodate kubectl --sort-by=.status.updatedReplicas get deployment # available kubectl --sort-by=.metadata.availableReplicas get deployment # containers kubectl --sort-by=.spec.template.spec.containers[*].name get deployment # images kubectl --sort-by=.spec.template.spec.containers[*].image get deployment ##### service # name kubectl --sort-by=.metadata.name get service # age kubectl --sort-by=.metadata.creationTimestamp get service # type kubectl --sort-by=.spec.type get service # clusterip kubectl --sort-by=.spec.clusterIP get service # port kubectl --sort-by=.spec.ports[*].port get service 参考：
kubectl Cheat Sheet AATHITH/kubesort # This a Bash Script that will help you forget the kubectl&rsquo;s default, difficult to remember, sorting feature by making it simpler.
命令 # autoscale # 参考 # kubectl Cheat Sheet kubectl commands `}),e.add({id:28,href:"/study-kubernetes/docs/advanced/debug/pod/",title:"pod 排错",section:"2.5 故障排查",content:` pod 排错 # 排查 Pod 异常的常用命令如下：
查看 Pod 状态：kubectl get pods &lt;pod-name&gt; -n &lt;namespace&gt; -o wide 查看 Pod 的 yaml 配置：kubectl get pods &lt;pod-name&gt; -n &lt;namespace&gt; -o yaml 查看 Pod 的事件：kubectl describe pods &lt;pod-name&gt; -n &lt;namespace&gt; 查看 Pod 容器日志：kubectl logs -n &lt;namespace&gt; &lt;pod-name&gt; [-c &lt;container-name&gt;] Pending 状态 # Pending 状态说明 Pod 还没有调度到某个 Node 上面
可以通过 kubectl describe pods &lt;pod-name&gt; -n &lt;namespace&gt; 命令查看到 Pod 的事件
参考：
Kubernetes 指南 - Pod 排错 排错指南 - Pod Kubernetes Docs - Troubleshoot Applications `}),e.add({id:29,href:"/study-kubernetes/docs/basic/concept/resource/service/",title:"Service",section:"API 对象",content:" Service # type # ClusterIP # 通过集群的内部 IP 暴露服务，选择该值，服务只能够在集群内部可以访问 这也是默认的 ServiceType NodePort # 通过每个 Node 上的 IP 和静态端口（NodePort）暴露服务 NodePort 服务会路由到 ClusterIP 服务，这个 ClusterIP 服务会自动创建 通过请求 &lt;NodeIP&gt;:&lt;NodePort&gt;，可以从集群的外部访问一个 NodePort 服务 LoadBalancer # 使用云提供商的负载局衡器，可以向外部暴露服务 外部的负载均衡器可以路由到 NodePort 服务和 ClusterIP 服务 访问方式 # 内部：ClusterIP:port (10.110.126.136:80) 外部： hostIP:nodePort (机器 IP:30032) EXTERNAL-IP:port (xxxx:80) # 没有 EXTERNAL-IP NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR istio-ingressgateway LoadBalancer 10.110.126.136 &lt;pending&gt; 15020:30332/TCP,80:30032/TCP,443:30551/TCP,31400:32430/TCP,15443:30294/TCP 13h app=istio-ingressgateway,istio=ingressgateway ExternalName # 通过返回 CNAME 和它的值，可以将服务映射到 externalName 字段的内容（例如， foo.bar.example.com） 没有任何类型代理被创建 CoreDNS 1.7 或更高版本才能使用 ExternalName 类型 "}),e.add({id:30,href:"/study-kubernetes/docs/advanced/eco/docker/command/",title:"命令",section:"Docker",content:" Docker 命令 # docker ps # docker ps vs docker container ls # Management Commands vs Commands\nDocker 1.13+ introduced grouped commands to help organize a bunch of Docker commands. Both commands do the same thing.\nFor example docker container ls is the new way to do docker ps.\nSure it’s more typing, but it’s a lot more clear on what it does. Likewise, now you can run docker image ls, docker network ls or docker volume ls. There’s consistency across all of these commands.\n参考：\nDifference between docker ps vs docker container ls docker attach vs docker exec # $ sudo docker attach 665b4a1e17b6 #by ID or $ sudo docker attach loving_heisenberg #by Name $ root@665b4a1e17b6:/# $ sudo docker exec -i -t 665b4a1e17b6 /bin/bash #by ID or $ sudo docker exec -i -t loving_heisenberg /bin/bash #by Name $ root@665b4a1e17b6:/# docker attach isn&rsquo;t for running an extra thing in a container, it&rsquo;s for attaching to the running process. attach 只有一个实例 docker exec is specifically for running new things in a already started container, be it a shell or some other process. exec 可以启动多个 参考：\ndifference between docker attach and docker exec How to get bash or ssh into a running container in background mode? "}),e.add({id:31,href:"/study-kubernetes/docs/advanced/eco/docker/version/",title:"版本",section:"Docker",content:` Docker 版本 # 查看版本 # docker -v docker --version #查看版本 docker-compose --version #查看版本 docker-machine --version #查看版本 docker version #查看client和server端版本，并可以查看是否开启体验功能 历史版本 # 最新版本
19.03 (2019-07-22) # 19.03.12 (2020-06-18) # 18.09 (2018-11-08) # Docker 合并版本号 # New in 18.09 is an aligned release model for Docker Engine - Community and Docker Engine - Enterprise. The new versioning scheme is YY.MM.x where x is an incrementing patch version. The enterprise engine is a superset of the community engine. They will ship concurrently with the same x patch version based on the same code base.
分拆 client 与 daemon # The client and container runtime are now in separate packages from the daemon in Docker Engine 18.09. Users should install and update all three packages at the same time to get the latest patch releases.
For example, on Ubuntu:
sudo apt install docker-ce docker-ce-cli containerd.io 18.06 (2018-07-18) # 17.03 (2017-03-01) # Docker CE 和 Docker EE（17.03） # 新版本: Docker Engine release notes
在 2017 年 3 月 2 日，docker 团队宣布企业版 Docker Enterprise Edition (EE) 发布。
为了一致，免费的 Docker Engine 改名为 Docker Community Edition (CE), 并且采用基于时间的版本号方案。
就在这一天，Docker EE 和 Docker CE 的 17.03 版本发布，这也是第一个采用新的版本号方案的版本。
Docker CE/EE 每个季度发布一次 季度版本 , 也就是说每年会发布 4 个季度版本，17.03, 17.06, 17.09, 17.12 就是 2017 年的 4 个季度版本的版本号。
同时 Docker CE 每个月还会发布一个 EDGE 版本，比如 17.04, 17.05, 17.07, 17.08, 17.10, 17.11。
Docker CE 季度版本自发布后会有 4 个月的维护期。
在基于时间的发布方案中，版本号格式为: YY.MM.，YY.MM 代表年月，patch 代表补丁号，从 0 开始，
在季度版本 (如 17.03) 的维护期内，bug 修复相关的更新会以 patch 递增的方式发布，
比如 17.03.0 -&gt; 17.03.1 -&gt; 17.03.2.
1.13.1 之前 # Docker Engine（0.1.0 到 1.13.1） # Docker CE 在 1.13.1 及之前版本叫 Docker Engine 。
版本说明参考：Docker Engine release notes (Previous versions)，
可以看到 Docker Engine 的版本号范围: 0.1.0 (2013-03-23) ~ 1.13.1 (2017-02-08)
1.11 # Docker 守护进程启动命令：
1.7: docker -d 1.8: docker daemon 1.11: dockerd 1.8 # 1.7 # `}),e.add({id:32,href:"/study-kubernetes/docs/basic/other/version/",title:"版本",section:"1.4 其他",content:` Kubernetes 版本 # kubernetes/kubernetes 查看版本 # kuberctl # kuberctl version 这里 Client Version 的 GitVersion 是 kuberctl 的版本
apiserver # kuberctl version 这里 Server Version 的 GitVersion 是 apiserver 的版本
kubelet # kuberctl get nodes 这里的 VERSION 是 kubelet 的版本
版本列表 # v1.26 # v1.25 # Moved container registry service from k8s.gcr.io to registry.k8s.io Introduced KMS v2 Kube-proxy images are now based on distroless images PodSecurityPolicy is Removed, Pod Security Admission graduates to Stable Ephemeral Containers Graduate to Stable Support for cgroups v2 Graduates to Stable Promoted endPort in Network Policy to Stable Promoted Local Ephemeral Storage Capacity Isolation to Stable Promoted core CSI Migration to Stable Promoted CSI Ephemeral Volume to Stable Promoted SeccompDefault to Beta Promoted CRD Validation Expression Language to Beta Promoted Server Side Unknown Field Validation to Beta v1.24 # v1.23 # v1.22 # v1.21 # v1.20 # v1.19 # v1.18 # Kubernetes 1.18 中包含 38 项功能增强 15 项为稳定版功能 11 项 beta 版功能 12 项 alpha 版功能 v1.17 # v1.16 # v1.15 # v1.14 (2019-03-26) # v1.13 (2018-12-03) # v1.12 (2018-09-27) # v1.11 (2018-06-27) # v1.10 (2018-03-26) # v1.9 (2017-12-15) # v1.8 (2017-09-28) # v1.7 (2017-06-29) # CRD (Custom Resource Definition) v1.6 # v1.5 # CRI (Container Runtime Interface) v1.0 # 参考 # `}),e.add({id:33,href:"/study-kubernetes/docs/advanced/eco/docker/port/",title:"端口",section:"Docker",content:` Docker 端口 # 端口映射 # -P（大写）：将容器内部开放的网络端口随机映射到宿主机的一个端口上 -p（小写）：指定要映射的端口，一个指定端口上只可以绑定一个容器 可以有多个 -p 端口映射格式 # # 指定 ip、指定宿主机 port、指定容器 port ip:hostport:containerport # 指定 ip、未指定宿主机 port（随机）、指定容器 port ip::containerport # 未指定 ip、指定宿主机 port、指定容器 port hostport:containerport 端口映射命令 # # 将容器暴露的所有端口，都随机映射到宿主机上（不推荐） docker run -P -it ubuntu /bin/bash # 将容器指定端口随机映射到宿主机一个随机端口 docker run -P 80 -it ubuntu /bin/bash # 将容器指定端口，随机映射到宿主机的指定 ip 的随机端口 # 有两个冒号 : docker run -P 192.168.0.100::80 -it ubuntu /bin/bash # 将容器指定端口指定映射到宿主机的指定端口上 docker run -p 8000:80 -it ubuntu /bin/bash # 将容器指定端口，指定映射到宿主机指定 ip 和端口 # 只能访问 192.168.0.100:8000， # 访问 127.0.0.1:8000 或 localhost:8000 都不行 docker run -p 192.168.0.100:8000:80 -it ubuntu /bin/bash 指定通信协议，比如 tcp、udp # # tcp docker run -ti -d --name my-nginx5 -p 8099:80/tcp docker.io/nginx # udp docker run -ti -d --name my-nginx6 -p 192.168.10.214:8077:80/udp docker.io/nginx 查看映射端口配置 # # 结果输出： 80/tcp -&gt; 0.0.0.0:800 docker port container_ID # 容器 ID 通过宿主机的 iptables 进行 nat 转发 # 容器除了在启动时添加端口映射关系，还可以通过宿主机的 iptables 进行 nat 转发，将宿主机的端口映射到容器的内部端口上， 这种方式适用于容器启动时没有指定端口映射的情况。
# 容器 my-nginx9 在启动时没有指定其内部的 80 端口映射到宿主机的端口上， # 所以默认是没法访问的 docker run -ti -d --name my-nginx9 docker.io/nginx # 首先获得容器的 ip 地址 docker inspect my-nginx9|grep IPAddress # 将容器的 80 端口映射到 dockers 宿主机的 9998 端口 iptables -t nat -A PREROUTING -p tcp -m tcp --dport 9998 -j DNAT --to-destination 172.17.0.9:80 iptables -t nat -A POSTROUTING -d 172.17.0.9/32 -p tcp -m tcp --sport 80 -j SNAT --to-source 192.16.10.214 iptables -t filter -A INPUT -p tcp -m state --state NEW -m tcp --dport 9998 -j ACCEPT # 保存以上 iptables 规则 iptables-save &gt; /etc/sysconfig/iptables # 查看/etc/sysconfig/iptables 文件， # 注意下面两行有关 icmp-host-prohibited 的设置一定要注释掉，否则 nat 转发会失败 # -A INPUT -j REJECT --reject-with icmp-host-prohibited # -A FORWARD -j REJECT --reject-with icmp-host-prohibited # 最后重启 iptbales 服务 systemctl restart iptables # 查看 iptables 规则 iptables -L # 然后访问 http://192.168.10.214:9998/，就能转发访问到 my-nginx9 容器的 80 端口了 问答 # 启动 docker 容器时，有如下报错 # /usr/bin/docker-current: Error response from daemon: driver failed programming external connectivity on endpoint my-nginx (db5a0edac68d1ea7ccaa3a1e0db31ebdf278076ef4851ee4250221af6167f9ac): (iptables failed: iptables --wait -t nat -A DOCKER -p tcp -d 0/0 --dport 8088 -j DNAT --to-destination 172.17.0.2:80 ! -i docker0: iptables: No chain/target/match by that name. 解决办法 1）不需要关闭防火墙 2）重启 docker 服务:systemctl restart docker 3）docker 服务重启后，所有容器都会关闭，应立即批量启动全部容器:docker start docker ps -a -q 启动的容器也会包括上面报错的容器，重启 docker 后，该容器就能正常启动和使用了。
Docker 端口映射到宿主机后，外部无法访问对应宿主机端口 # 创建 docker 容器的时候,做了端口映射到宿主机, 防火墙已关闭, 但是外部始终无法访问宿主机端口? 这种情况基本就是因为宿主机没有开启 ip 转发功能，从而导致外部网络访问宿主机对应端口是没能转发到 Docker Container 所对应的端口上。
解决办法: Linux 发行版默认情况下是不开启 ip 转发功能的。这是一个好的做法，因为大多数人是用不到 ip 转发的，但是如果架设一个 Linux 路由或者 VPN 服务我们就需要开启该服务了。
在 Linux 中开启 ip 转发的内核参数为：net.ipv4.ip_forward，查看是否开启 ip 转发：
cat /proc/sys/net/ipv4/ip_forward // 0：未开启，1：已开启 打开 ip 转发功能 # 临时打开 ip 转发功能!
# echo 1 &gt; /proc/sys/net/ipv4/ip_forward # sysctl -w net.ipv4.ip_forward=1 永久生效的 ip 转发
vim /etc/sysctl.conf net.ipv4.ip_forward = 1 # 立即生效 sysctl -p /etc/sysctl.conf Linux 系统中也可以通过重启网卡来立即生效。
修改 sysctl.conf 文件后的生效
# CentOS 6 service network restart # CentOS 7 systemctl restart network 参考：
Docker 容器内部端口映射到外部宿主机端口 - 运维笔记 `}),e.add({id:34,href:"/study-kubernetes/docs/advanced/eco/docker/image/",title:"镜像",section:"Docker",content:` Docker 镜像 # dangling 镜像 # 未被任何镜像引用的镜像
docker push # 上传到 私有仓库 # docker build -t human-attribute:20200415_1585818439123 . docker tag human-attribute:20200415_1585818439123 ote-harbor.baidu.com/aiedge/human-attribute:20200415_1585818439123 docker push ote-harbor.baidu.com/aiedge/human-attribute:20200415_1585818439123 `}),e.add({id:35,href:"/study-kubernetes/docs/advanced/eco/docker/containerd/",title:"containerd",section:"Docker",content:" containerd # "}),e.add({id:36,href:"/study-kubernetes/docs/advanced/eco/docker/dockerfile/",title:"dockerfile",section:"Docker",content:` dockerfile # 目标:
更快的构建速度 更小的 Docker 镜像大小 更少的 Docker 镜像层 充分利用镜像缓存 增加 Dockerfile 可读性 让 Docker 容器使用起来更简单 做法：
编写 .dockerignore 文件
容器只运行单个应用
将多个 RUN 指令合并为一个
基础镜像的标签不要用 latest
如果你的确需要使用最新版的基础镜像，可以使用 latest 标签，否则的话，最好指定确定的镜像标签 每个 RUN 指令后删除多余文件
# 假设我们更新了 apt-get 源，下载，解压并安装了一些软件包，它们都保存在 /var/lib/apt/lists/ 目录中 RUN apt-get update \\ &amp;&amp; apt-get install -y nodejs \\ # added lines &amp;&amp; rm -rf /var/lib/apt/lists/* 选择合适的基础镜像 (alpine 版本最好)
alpine 是一个极小化的 Linux 发行版，只有 4MB，这让它非常适合作为基础镜像 设置 WORKDIR 和 CMD
FROM node:7-alpine # WORKDIR 指令可以设置默认目录，也就是运行 RUN / CMD / ENTRYPOINT 指令的地方。 WORKDIR /app COPY . /app RUN npm install # CMD 指令可以设置容器创建是执行的默认命令。另外，你应该讲命令写在一个数组中，数组中每个元素为命令的每个单词。 CMD [&#34;npm&#34;, &#34;start&#34;] 使用 ENTRYPOINT (可选)
在 entrypoint 脚本中使用 exec
优先使用 COPY，不使用 ADD
合理调整 COPY 与 RUN 的顺序
把变化最少的部分放在 Dockerfile 的前面，这样可以充分利用镜像缓存 # 源代码会经常变化，则每次构建镜像时都需要重新安装 NPM 模块，这显然不是我们希望看到的。 # 因此我们可以先拷贝 package.json，然后安装 NPM 模块，最后才拷贝其余的源代码。 # 这样的话，即使源代码变化，也不需要重新安装 NPM 模块 FROM node:7-alpine WORKDIR /app COPY package.json /app RUN npm install COPY . /app ENTRYPOINT [&#34;./entrypoint.sh&#34;] CMD [&#34;start&#34;] 设置默认的环境变量，映射端口和数据卷 使用 LABEL 设置镜像元数据 添加 HEALTHCHECK 参考：
Best practices for writing Dockerfiles Dockerfile 最佳实践 如何编写最佳的 Dockerfile `}),e.add({id:37,href:"/study-kubernetes/docs/advanced/eco/kata/",title:"Kata",section:"2.8 生态",content:` Kata # Kata Containers is an open source project and community working to build a standard implementation of lightweight Virtual Machines (VMs) that feel and perform like containers, but provide the workload isolation and security advantages of VMs.
https://katacontainers.io/
v1 # kata-containers/runtime v2 # kata-containers/kata-containers 教程 # `}),e.add({id:38,href:"/study-kubernetes/docs/advanced/eco/kubevirt/",title:"KubeVirt",section:"2.8 生态",content:` KubeVirt # kubevirt/kubevirt Kubernetes Virtualization API and runtime in order to define and manage virtual machines.
https://kubevirt.io/
教程 # `}),e.add({id:39,href:"/study-kubernetes/docs/basic/concept/",title:"1.2 概念",section:"第一部分 基础入门",content:" 概念 # "}),e.add({id:40,href:"/study-kubernetes/docs/advanced/tool/",title:"2.2 工具",section:"第二部分 进阶实战",content:" 工具 # "}),e.add({id:41,href:"/study-kubernetes/docs/design/crd/",title:"3.2 CRD",section:"第三部分 设计与实现",content:" CRD # "}),e.add({id:42,href:"/study-kubernetes/docs/design/crd/operator/",title:"3.2.2 Operator",section:"3.2 CRD",content:" Operator # "}),e.add({id:43,href:"/study-kubernetes/docs/appendix/interview/",title:"4.2 面试题",section:"第四部分 附录",content:" 面试题 # 基础 # 进阶 # 其他 # 头条面试题 # 头条人选一（最近一周刚面试，面试岗位是头条杭州的 docker、k8s 工程师） 1.解释一下 acid 2.数据库的隔离级别 3.每个隔离级别的实现原理 4.讲一下分布式事务现有的方案，优缺点 5.说一下 cgroup 原理 6.说一下 mesos，k8s 的架构 7.说一下 actor 编程模型的原理和意义 算法：输出所有出现次数大于 n/k 的数,如果没有这样的数,请输出”-1“。 人选反馈：面试官会抓住一个点一直追问，问的比较细。 头条人选二（两周前面试 paas 平台开发工程师） 一面 1.字符串原地反转 2.软连接和硬连接的区别 3.前序和中序构造二叉树 4.10 亿个数字排序 5.大型企业的上线流程 二面 连续子数组的最大和 链表回文 三面 基于人选项目去发问，偏业务，没有算法题，还问到一些开源组件的使用情况。 头条人选三（面试基础架构部门容器岗位） 1. 系统 （Linux 下面进程是如何调度的，进程的优先级），有没有遇到过问题，描述原因和解决方法 2.网络 TIME_WAIT 状态出现的原因，排查的思路，有没有调优的方法 2. 数据库 MyISAM 和 InnoDB 的简单区别 3. Docker/K8s CGroup 如何做 cpu 的资源限制，哪几种限制方式 K8s 中的 request 和 limit 是如何实现的 K8s 中的 rc、rs、deployment 的区别，特点 5.算法 合并两个有序链表 头条人选四（基础架构部门研发） docker pull 镜像原理 registry 镜像存储原理 k8s deployment 创建过程 gpu 资源共享 scheduler 工作原理 docker 架构 docker exec 过程 goroutine 挂起 实现阻塞队列 项目 linux 进程调度 http 请求过程 tcp 建立连接和断开连接 goroutine 原理 flannel 工作原理 头条人选五（devops 偏容器的人选） 服务器负载是怎么计算的 如何查看服务执行卡住时服务器做了哪些事 查看进程打开哪些文件，查看哪些进程打开文件多 docker 网络模式有几种 分别是做什么 浏览器输入域名到返回结果过程 DNS 使用了什么协议，为什么要用这种协议 tcp 协议如何保证传输安全 TIME-WAIT 状态的前后过程以及 TIME-WAIT 过多时怎么处理 怎么优化 K8S 是怎么调度的 iptables 主要有哪些链和哪些表，分别是做什么用 docker 会调用 iptables 的哪些链 常驻空间和虚拟空间有什么区别 网络加速或者网络优化是怎么来做的，服务端和客户端分别可以做哪些 动态 CDN 和静态 CDN 的区别 算法题： 求两个文本文件的交集，并输出，比如 ABC 在 a.txt 有 5 行，在 b.txt 有 3 行 那么在结果文件中输出 3 行 "}),e.add({id:44,href:"/study-kubernetes/docs/basic/arch/network/dns/kubedns/",title:"KubeDNS",section:"K8s DNS",content:` KubeDNS # 在 Linux 系统中，/etc/resolv.conf 是存储 DNS 服务器的文件， 普通 Pod 的 /etc/resolv.conf 文件应该存储的是 kube-dns 的 Service IP。
nameserver 10.99.0.2 # 这里存储的是 kube-dns 的 Service IP search default.svc.cluster.local. svc.cluster.local. cluster.local. options ndots:5 如何进入 kube-dns 容器进行抓包 # DNS 容器往往不具备 bash，所以不能通过 docker exec 或者 kubectl exec 的方式进入容器抓包。
docker inspect --format &#34;{{.State.Pid}}&#34; dns_container_id # 进入 container 的 network namespace nsenter -n -t pid # 对 53 端口进行抓包 tcpdump -i eth0 -N udp dst port 53 dnsPolicy # ClusterFirst（默认） # 优先使用 kubedns 或者 coredns 进行域名解析。 如果解析不成功，才会使用宿主机的 DNS 配置进行解析。
ClusterFirstWithHostNet # 当一个 Pod 以 HOST 模式（和宿主机共享网络，hostNetwork: true）启动时，这个 POD 中的所有容器都会使用宿主机的 /etc/resolv.conf 配置进行 DNS 查询。 但是如果在 Pod 中仍然还想继续使用 k8s 集群 的 DNS 服务时，就需要将 dnsPolicy 设置为 ClusterFirstWithHostNet。
Default # 让 kubelet 来决定 Pod 内的 DNS 使用哪种 DNS 策略。 kubelet 的默认方式，其实就是使用宿主机的 /etc/resolv.conf 来进行解析。 你可以通过设置 kubelet 的启动参数， --resolv-conf=/etc/resolv.conf 来决定该 DNS 服务使用的解析文件的地址
当我们部署集群 DNS 服务的时候，一般就需要将 dnsPolicy 设置成 Default， 而并非使用默认值 ClusterFirst，否则该 DNS 服务的上游解析地址会变成它自身的 Service 的 ClusterIP（我解析我自己），导致域名无法解析。
None # 不会使用集群和宿主机的 DNS 策略，而是和 dnsConfig 配合一起使用，来自定义 DNS 配置，否则在提交修改时报错。
kube-dns 组成 # kubedns # 依赖 client-go 中的 informer 机制监视 k8s 中的 Service 和 Endpoint 的变化，并将这些结构维护进内存来服务内部 DNS 解析请求。
dnsmasq # 区分 Domain 是集群内部还是外部，给外部域名提供上游解析，内部域名发往 10053 端口，并将解析结果缓存，提高解析效率。
sidecar # 对 kubedns 和 dnsmasq 进行健康检查和收集监控指标。
如何调试 DNS 解析 # 参考 k8smeetup：调试 DNS 解析
`}),e.add({id:45,href:"/study-kubernetes/docs/advanced/tool/kustomize/",title:"Kustomize",section:"2.2 工具",content:` Kustomize # kubernetes-sigs/kustomize Customization of kubernetes YAML configurations
kustomize lets you customize raw, template-free YAML files for multiple purposes, leaving the original YAML untouched and usable as is.
一般应用都会存在多套部署环境：开发环境、测试环境、生产环境，多套环境意味着存在多套 K8S 应用资源 YAML。 而这么多套 YAML 之间只存在微小配置差异，比如镜像版本不同、Label 不同等，而这些不同环境下的 YAML 经常会因为人为疏忽导致配置错误。 再者，多套环境的 YAML 维护通常是通过把一个环境下的 YAML 拷贝出来然后对差异的地方进行修改。 一些类似 Helm 等应用管理工具需要额外学习 DSL 语法。
总结以上，在 k8s 环境下存在多套环境的应用，经常遇到以下几个问题：
如何管理不同环境或不同团队的应用的 Kubernetes YAML 资源 如何以某种方式管理不同环境的微小差异，使得资源配置可以复用，减少 copy and change 的工作量 如何简化维护应用的流程，不需要额外学习模板语法 Kustomize 通过以下几种方式解决了上述问题：
kustomize 通过 Base &amp; Overlays 方式 (下文会说明) 方式维护不同环境的应用配置 kustomize 使用 patch 方式复用 Base 配置，并在 Overlay 描述与 Base 应用配置的差异部分来实现资源复用 kustomize 管理的都是 Kubernetes 原生 YAML 文件，不需要学习额外的 DSL 语法 `}),e.add({id:46,href:"/study-kubernetes/docs/basic/arch/architecture/",title:"架构",section:"1.3 架构",content:" Kubernetes 架构 # "}),e.add({id:47,href:"/study-kubernetes/docs/basic/arch/",title:"1.3 架构",section:"第一部分 基础入门",content:" 架构 # "}),e.add({id:48,href:"/study-kubernetes/docs/advanced/feature/",title:"2.3 功能",section:"第二部分 进阶实战",content:` 功能 # 存储 # rook/rook # Storage Orchestration for Kubernetes
Name Details API Group Status Rook Framework The framework for common storage specs and logic used to support other storage providers. rook.io/v1alpha2 Alpha Ceph Ceph is a distributed storage system that provides file, block and object storage and is deployed in large scale production clusters. ceph.rook.io/v1 Stable CockroachDB CockroachDB is a cloud-native SQL database for building global, scalable cloud services that survive disasters. cockroachdb.rook.io/v1alpha1 Alpha Cassandra Cassandra is a highly available NoSQL database featuring lightning fast performance, tunable consistency and massive scalability. Scylla is a close-to-the-hardware rewrite of Cassandra in C++, which enables much lower latencies and higher throughput. cassandra.rook.io/v1alpha1 Alpha EdgeFS EdgeFS is high-performance and fault-tolerant decentralized data fabric with access to object, file, NoSQL and block. edgefs.rook.io/v1 Stable NFS Network File System (NFS) allows remote hosts to mount file systems over a network and interact with those file systems as though they are mounted locally. nfs.rook.io/v1alpha1 Alpha YugabyteDB YugabyteDB is a high-performance, cloud-native distributed SQL database which can tolerate disk, node, zone and region failures automatically. yugabytedb.rook.io/v1alpha1 Alpha `}),e.add({id:49,href:"/study-kubernetes/docs/appendix/attention/",title:"4.3 关注项目",section:"第四部分 附录",content:` Kubernetes 关注项目 # ramitsurana/awesome-kubernetes kubernetes 官方 # kubernetes/kubernetes # Production-Grade Container Scheduling and Management https://kubernetes.io
kubernetes/client-go # Go client for Kubernetes.
kubernetes/kops # kops - Kubernetes Operations The easiest way to get a production grade Kubernetes cluster up and running.
kubernetes/dashboard # General-purpose web UI for Kubernetes clusters
kubernetes/kube-state-metrics # Add-on agent to generate and expose cluster-level metrics.
kubernetes/examples # Kubernetes application example tutorials
kubernetes/sample-controller # Repository for sample controller. Complements sample-apiserver
kubernetes/node-problem-detector # This is a place for various problem detectors running on the Kubernetes nodes.
kubernetes/enhancements # Enhancements tracking repo for Kubernetes
kubernetes/test-infra # This repository contains tools and configuration files for the testing and automation needs of the Kubernetes project.
kubernetes/kompose # Go from Docker Compose to Kubernetes http://kompose.io
kubernetes/ingress-nginx # NGINX Ingress Controller for Kubernetes https://kubernetes.github.io/ingress-nginx/
Kubernetes 周边工具 # rancher/rancher # Rancher 是一个容器管理平台，通过 Rancher 可以实现 Docker 和 Kubernetes 的轻松部署。
helm/helm # The Kubernetes Package Manager https://helm.sh
rancher/k3s # Lightweight Kubernetes. 5 less than k8s. https://k3s.io
kubernetes-sigs/kustomize # Customization of kubernetes YAML configurations
fluxcd/flagger # Flagger implements several deployment strategies (Canary releases, A/B testing, Blue/Green mirroring) using a service mesh (App Mesh, Istio, Linkerd) or an ingress controller (Contour, Gloo, NGINX, Skipper, Traefik) for traffic routing. For release analysis, Flagger can query Prometheus, Datadog, New Relic or CloudWatch and for alerting it uses Slack, MS Teams, Discord and Rocket.
Flagger is a Cloud Native Computing Foundation project and part of Flux family of GitOps tools.
rancher/k3os # Purpose-built OS for Kubernetes, fully managed by Kubernetes. https://k3os.io
rancher/fleet # Manage large fleets of Kubernetes clusters
baidu/ote-stack # OTE-Stack is an edge computing platform for 5G and AI https://ote.baidu.com/
其他 # knative/serving # Kubernetes-based, scale-to-zero, request-driven compute https://knative.dev/docs/serving
kubeflow/kubeflow # Machine Learning Toolkit for Kubernetes
alauda/kube-ovn # A Kubernetes Network Fabric for Enterprises that is Rich in Functions and Easy in Operations https://kube-ovn.io
OpenNESS # OpenNESS（Open Network Edge Services Software）是一个开源的边缘应用程序管理系统，使服务提供商和企业能够在任何网络的边缘上构建、部署和操作自己的边缘应用程序（ME APP），支持通过简易的方式将运行在 Telco/Public Cloud 中的 APP 迁移到边缘。
OpenNESS, the easy button to deploy innovative services at the Edge. OpenNESS is an open source reference toolkit that makes it easy to move applications from the Cloud to the Network and On-Premise Edge.
istio/istio # Connect, secure, control, and observe services. https://istio.io
dapr/dapr # Dapr is a portable, event-driven, runtime for building distributed applications across cloud and edge.
`}),e.add({id:50,href:"/study-kubernetes/docs/advanced/network/calico/",title:"Calico",section:"Kubernetes 网络",content:` Calico # projectcalico/calico https://docs.projectcalico.org/
calico/kube-controllers # `}),e.add({id:51,href:"/study-kubernetes/docs/advanced/feature/operator/etcd/",title:"etcd Operator",section:"Operator",content:` etcd Operator # $ git clone https://github.com/coreos/etcd-operator # 因为，Etcd Operator 需要访问 Kubernetes 的 APIServer 来创建对象 # 为 Etcd Operator 创建 RBAC 规则 $ example/rbac/create_role.sh Etcd Operator 本身，其实就是一个 Deployment
而一旦 Etcd Operator 的 Pod 进入了 Running 状态，你就会发现，有一个 CRD 被自动创建了出来
这个 CRD 名叫 etcdclusters.etcd.database.coreos.com
实际上是在 Kubernetes 里添加了一个名叫 EtcdCluster 的自定义资源类型（CRD）。
而 Etcd Operator 本身，就是这个 CRD 对应的自定义控制器。
apiVersion: extensions/v1beta1 kind: Deployment metadata: name: etcd-operator spec: replicas: 1 template: metadata: labels: name: etcd-operator spec: containers: - name: etcd-operator image: quay.io/coreos/etcd-operator:v0.9.2 command: - etcd-operator env: - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name 当 Etcd Operator 部署好之后，接下来在这个 Kubernetes 里创建一个 Etcd 集群的工作就非常简单了。
你只需要编写一个 EtcdCluster 的 YAML 文件，然后把它提交给 Kubernetes
$ kubectl apply -f example/example-etcd-cluster.yaml example-etcd-cluster.yaml 定义的是 EtcdCluster 这个 CRD 的一个具体实例， 也就是一个 Custom Resource（CR）
EtcdCluster 的 spec 字段非常简单。其中，size=3 指定了它所描述的 Etcd 集群的节点个数。 而 version=“3.2.13”，则指定了 Etcd 的版本，仅此而已。
而真正把这样一个 Etcd 集群创建出来的逻辑，就是 Etcd Operator 要实现的主要工作
apiVersion: &#34;etcd.database.coreos.com/v1beta2&#34; kind: &#34;EtcdCluster&#34; metadata: name: &#34;example-etcd-cluster&#34; spec: size: 3 version: &#34;3.2.13&#34; `}),e.add({id:52,href:"/study-kubernetes/docs/advanced/tool/helm/",title:"Helm",section:"2.2 工具",content:" HELM # helm/helm Helm is the best way to find, share, and use software built for Kubernetes.\n官网：https://helm.sh\nHelm is a tool for managing Charts. Charts are packages of pre-configured Kubernetes resources.\nUse Helm to:\nFind and use popular software packaged as Helm Charts to run in Kubernetes Share your own applications as Helm Charts Create reproducible builds of your Kubernetes applications Intelligently manage your Kubernetes manifest files Manage releases of Helm packages 基本概念 # Chart：一个 Helm 包，其中包含了运行一个应用所需要的镜像、依赖和资源定义等，还可能包含 Kubernetes 集群中的服务定义， Chart 是用来封装 Kubernetes 原生应用程序的一系列 YAML 文件集合 类似 Homebrew 中的 formula apt 的 dpkg yum/dnf 的 rpm Repository：用于发布和存储 Chart 的存储库。 Release：在 Kubernetes 集群上运行的 Chart 的一个实例。在同一个集群上，一个 Chart 可以安装很多次。每次安装都会创建一个新的 release。例如一个 MySQL Chart，如果想在服务器上运行两个数据库，就可以把这个 Chart 安装两次。每次安装都会生成自己的 Release，会有自己的 Release 名称。 Tiller 是 Helm2 的服务端，通常运行在您的 kubernetes 集群中。Tiller 用于接收 Helm 的请求，并根据 Chart 生成 Kubernetes 的部署文件，然后提交给 Kubernetes 创建应用。 HElm3 中删除了 Tiller，只有客户端使用，调用 ~/.kube/config 来访问 api server 进行创建 命令 # completion generate autocompletions script for the specified shell create create a new chart with the given name dependency manage a chart&#39;s dependencies env helm client environment information get download extended information of a named release help Help about any command history fetch release history install install a chart lint examine a chart for possible issues list list releases package package a chart directory into a chart archive plugin install, list, or uninstall Helm plugins pull download a chart from a repository and (optionally) unpack it in local directory repo add, list, remove, update, and index chart repositories rollback roll back a release to a previous revision search search for a keyword in charts show show information of a chart status display the status of the named release template locally render templates test run tests for a release uninstall uninstall a release upgrade upgrade a release verify verify that a chart at the given path has been signed and is valid version print the client version information 开源镜像 # BurdenBear/kube-charts-mirror # # 删除默认的源 helm repo remove stable helm repo remove incubator # 增加新的国内镜像源 helm repo add stable http://mirror.azure.cn/kubernetes/charts/ helm repo add incubator http://mirror.azure.cn/kubernetes/charts-incubator/ helm repo list helm search repo mysql TODO # # Initialize a Helm Chart Repository $ helm repo add stable https://kubernetes-charts.storage.googleapis.com/ # list the charts you can install $ helm search repo stable # Install an Example Chart $ helm repo update $ helm install stable/mysql --generate-name NAME: mysql-1596179301 LAST DEPLOYED: Fri Jul 31 15:08:27 2020 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: MySQL can be accessed via port 3306 on the following DNS name from within your cluster: mysql-1596179301.default.svc.cluster.local To get your root password run: MYSQL_ROOT_PASSWORD=$(kubectl get secret --namespace default mysql-1596179301 -o jsonpath=&#34;{.data.mysql-root-password}&#34; | base64 --decode; echo) To connect to your database: 1. Run an Ubuntu pod that you can use as a client: kubectl run -i --tty ubuntu --image=ubuntu:16.04 --restart=Never -- bash -il 2. Install the mysql client: $ apt-get update &amp;&amp; apt-get install mysql-client -y 3. Connect using the mysql cli, then provide your password: $ mysql -h mysql-1596179301 -p To connect to your database directly from outside the K8s cluster: MYSQL_HOST=127.0.0.1 MYSQL_PORT=3306 # Execute the following command to route the connection: kubectl port-forward svc/mysql-1596179301 3306 mysql -h ${MYSQL_HOST} -P${MYSQL_PORT} -u root -p${MYSQL_ROOT_PASSWORD} "}),e.add({id:53,href:"/study-kubernetes/docs/advanced/eco/k3s/",title:"K3s",section:"2.8 生态",content:" K3s # rancher/k3s 教程 # "}),e.add({id:54,href:"/study-kubernetes/docs/advanced/eco/oam/kubevela/",title:"KubeVela",section:"OAM",content:` KubeVela # oam-dev/kubevela Make shipping applications more enjoyable. https://kubevela.io/
ps: oam-dev/rudr 已经被废弃，转为 KubeVela
`}),e.add({id:55,href:"/study-kubernetes/docs/advanced/eco/oam/",title:"OAM",section:"2.8 生态",content:` OAM (Open Application Model) # OAM 是阿里巴巴和微软共同开源的云原生应用规范模型
Rudr 的应用程序有三个元素：Components（组件）、Configuration（配置）、Traits（特征）：
组件定义一个或多个面向操作系统的容器镜像以及硬件需求，如 CPU、内存和存储等 配置处理运行时的参数，比如环境变量 特征声明运行时的属性，比如 Volume、Ingress、伸缩等等。
2019 年 10 月宣布开源，同时开源了基于 OAM 的实现 Rudr。
oam-dev/spec # The Open Application Model specification
oam-dev/rudr # A Kubernetes implementation of the Open Application Model specification https://oam.dev
Rudr 已经被废弃，转为 KubeVela
oam-dev/kubevela # Make shipping applications more enjoyable. https://kubevela.io/
OpenTelemetry # Specifications for OpenTelemetry https://opentelemetry.io
参考：
OAM（开放应用模型）—— 定义云原生应用标准的野望 `}),e.add({id:56,href:"/study-kubernetes/docs/advanced/feature/operator/",title:"Operator",section:"2.3 功能",content:` Operator # Operator 的工作原理 # 利用了 Kubernetes 的自定义 API 资源（CRD），来描述我们想要部署的 “有状态应用”； 然后在自定义控制器里，根据自定义 API 对象的变化，来完成具体的部署和运维工作。
所以，编写一个 Etcd Operator，与编写一个自定义控制器的过程，没什么不同。
Operator 开发 # Kubebuilder # kubernetes-sigs/kubebuilder https://kubebuilder.io
operator-framework/operator-sdk # SDK for building Kubernetes applications. Provides high level APIs, useful abstractions, and project scaffolding.
https://sdk.operatorframework.io
kudobuilder/kudo # GoogleCloudPlatform/metacontroller # Lightweight Kubernetes controllers as a service https://metacontroller.app/
可与 Webhook 结合使用，以实现自己的功能。
operator-framework/operator-lifecycle-manager # A management framework for extending Kubernetes with Operators
Operator 示例 # coreos/etcd-operator # Etcd Operator # coreos/etcd-operator $ git clone https://github.com/coreos/etcd-operator # 因为，Etcd Operator 需要访问 Kubernetes 的 APIServer 来创建对象 # 为 Etcd Operator 创建 RBAC 规则 $ example/rbac/create_role.sh Etcd Operator 本身，其实就是一个 Deployment
而一旦 Etcd Operator 的 Pod 进入了 Running 状态，你就会发现，有一个 CRD 被自动创建了出来
这个 CRD 名叫 etcdclusters.etcd.database.coreos.com
实际上是在 Kubernetes 里添加了一个名叫 EtcdCluster 的自定义资源类型（CRD）。
而 Etcd Operator 本身，就是这个 CRD 对应的自定义控制器。
apiVersion: extensions/v1beta1 kind: Deployment metadata: name: etcd-operator spec: replicas: 1 template: metadata: labels: name: etcd-operator spec: containers: - name: etcd-operator image: quay.io/coreos/etcd-operator:v0.9.2 command: - etcd-operator env: - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name 当 Etcd Operator 部署好之后，接下来在这个 Kubernetes 里创建一个 Etcd 集群的工作就非常简单了。
你只需要编写一个 EtcdCluster 的 YAML 文件，然后把它提交给 Kubernetes
$ kubectl apply -f example/example-etcd-cluster.yaml example-etcd-cluster.yaml 定义的是 EtcdCluster 这个 CRD 的一个具体实例， 也就是一个 Custom Resource（CR）
EtcdCluster 的 spec 字段非常简单。其中，size=3 指定了它所描述的 Etcd 集群的节点个数。 而 version=“3.2.13”，则指定了 Etcd 的版本，仅此而已。
而真正把这样一个 Etcd 集群创建出来的逻辑，就是 Etcd Operator 要实现的主要工作
apiVersion: &#34;etcd.database.coreos.com/v1beta2&#34; kind: &#34;EtcdCluster&#34; metadata: name: &#34;example-etcd-cluster&#34; spec: size: 3 version: &#34;3.2.13&#34; zookeeper operator # 参考 # Introducing Operators: Putting Operational Knowledge into Software
Best practices for building Kubernetes Operators and stateful apps
如何看待 kubebuilder 与 Operator Framework (Operator SDK) ？
Integrating Kubebuilder and Operator SDK
`}),e.add({id:57,href:"/study-kubernetes/docs/design/crd/operator/operatorhub/",title:"OperatorHub",section:"3.2.2 Operator",content:` OperatorHub # https://operatorhub.io/
`}),e.add({id:58,href:"/study-kubernetes/docs/basic/concept/resource/volume/",title:"Volume",section:"API 对象",content:` Volume # Volume 必须和 pod 在同一个 namespace # All sources are required to be in the same namespace as the Pod.
参考：
kubernetes.io: volumes all-in-one volume design document `}),e.add({id:59,href:"/study-kubernetes/docs/basic/other/distro/",title:"发行版",section:"1.4 其他",content:` Kubernetes 发行版 # Rancher 偏运维 RKE Rancher Kubernetes Engine rancher/rke OpenShift 偏开发 AWS EKS Elastic Kubernetes Service Google GKE Google Kubernetes Engine Microsoft AKS Azure Kubernetes Service CDK Canonical Distribution of Kubernetes （Canonical 是 Ubuntu Linux 的制造商） CoreOS Tectonic/Red Hat CoreOS Docker 社区版 / Docker 企业版 Heptio Kubernetes 订阅版（Kubernetes 的两位创始人 Craig McLuckie 和 Joe Beda，创办了 Heptio） 2018 年 VMware 收购了 Heptio，不过此次收购目前暂未影响 Heptio 的产品计划 Kontena Pharos PKS Pivotal Container Service SUSE 容器服务平台 Telekube kubesphere/kubesphere KubeSphere 是在 Kubernetes 之上构建的以应用为中心的多租户容器平台，提供全栈的 IT 自动化运维的能力，简化企业的 DevOps 工作流。 KubeOperator/KubeOperator KubeOperator 是一个开源的轻量级 Kubernetes 发行版，专注于帮助企业规划、部署和运营生产级别的 K8s 集群。 参考：
10 Kubernetes distributions leading the container revolution 5 Reasons Not to Use Kubernetes Distributions Kubesphere 与 Rancher 有什么区别？ 部署生产级别的 Kubernetes 集群要注意哪些问题？ 如何在 CentOS7 上部署 Kubernetes 集群？ `}),e.add({id:60,href:"/study-kubernetes/docs/basic/arch/component/",title:"组件",section:"1.3 架构",content:" Kubernetes 组件 # "}),e.add({id:61,href:"/study-kubernetes/docs/basic/other/",title:"1.4 其他",section:"第一部分 基础入门",content:" 其他 # "}),e.add({id:62,href:"/study-kubernetes/docs/advanced/eco/docker/network/",title:"Docker 网络",section:"Docker",content:` Docker 网络 # 网络模式 # bridge 模式 使用 –net=bridge 指定，默认 host 模式 使用 –net=host 指定 一些对安全性有求高并且不需要联网的应用可以使用 none 网络 none 模式 使用 –net=none 指定 container 模式 使用 –net=container:NAMEorID 指定 bridge 模式 # Docker Daemon 启动时默认会创建 Docker0 这个网桥，网段为 172.17.0.0/16,
宿主机 IP 为 172.17.0.1 , 作为这个虚拟子网的网关。
创建网桥 # 新建一个名为 anyesu_net 网段为 172.18.0.0/16 的网桥：
docker network create --subnet=172.18.0.0/16 anyesu_net 启动容器时指定 --net anyesu_net 即可。
host 模式 # 优势 # host 网络最大的好处就是性能
劣势 # host 网络不便之处就是考虑端口冲突问题
参考：
Docker 下的网络模式 `}),e.add({id:63,href:"/study-kubernetes/docs/advanced/eco/kubeedge/",title:"KubeEdge",section:"2.8 生态",content:" KubeEdge # kubeedge/kubeedge 教程 # "}),e.add({id:64,href:"/study-kubernetes/docs/advanced/debug/",title:"2.5 故障排查",section:"第二部分 进阶实战",content:" 故障排查 # server localhost:8080 was refused # $ k get node The connection to the server localhost:8080 was refused - did you specify the right host or port? 参考 # 监控、日志和排错 "}),e.add({id:65,href:"/study-kubernetes/docs/advanced/eco/knative/",title:"Knative",section:"2.8 生态",content:` Knative # knative/serving Kubernetes-based, scale-to-zero, request-driven compute
https://knative.dev/docs/serving
Kubernetes 是容器平台，负责运行和管理容器 Knative 是代码平台，负责容器的构建、运行、扩展和路由 教程 # Getting Started with Knative - Building Modern Serverless Workloads on Kubernetes # Getting Started with Knative 是一本由 Pivotal 公司赞助 O’Reilly 出品的免费电子书
社区翻译版本：Knative 入门 —— 构建基于 Kubernetes 的现代化 Serverless 应用
`}),e.add({id:66,href:"/study-kubernetes/docs/advanced/eco/kubeflow/",title:"Kubeflow",section:"2.8 生态",content:` Kubeflow # kubeflow/kubeflow Machine Learning Toolkit for Kubernetes
教程 # `}),e.add({id:67,href:"/study-kubernetes/docs/advanced/prof/",title:"2.6 性能",section:"第二部分 进阶实战",content:" 性能 # "}),e.add({id:68,href:"/study-kubernetes/docs/advanced/eco/kubeless/",title:"Kubeless",section:"2.8 生态",content:` Kubeless # kubeless/kubeless Kubernetes Native Serverless Framework
`}),e.add({id:69,href:"/study-kubernetes/docs/advanced/test/test/",title:"2.7 测试",section:"第二部分 进阶实战",content:` 测试 # open-policy-agent/conftest # Write tests against structured configuration data using the Open Policy Agent Rego query language
`}),e.add({id:70,href:"/study-kubernetes/docs/advanced/tool/client-go/",title:"client-go",section:"2.2 工具",content:` kubernetes/client-go # 安装 # 版本 # v0.x.y # Kubernetes releases &gt;= v1.17.0
kubernetes-1.x.y # Kubernetes releases &lt; v1.17.0
`}),e.add({id:71,href:"/study-kubernetes/docs/design/code/1.25.0/",title:"1.25.0",section:"3.7 源码分析",content:" Kubernetes 1.25.0 源码分析 # https://github.com/kubernetes/kubernetes/tree/v1.25.0\n代码行数 # 185 万行\nXAMPPRocky/tokei $ tokei -V tokei 12.1.2 compiled with serialization support: json, cbor, yaml $ tokei --sort code --exclude vendor --exclude test =============================================================================== Language Files Lines Code Comments Blanks =============================================================================== Go 8825 2359999 1856340 310856 192803 JSON 535 561325 561325 0 0 YAML 786 121669 120814 660 195 Shell 228 34386 21884 8174 4328 Protocol Buffers 84 30402 8695 16987 4720 PowerShell 5 3810 2798 727 285 Makefile 10 1257 637 466 154 Python 5 656 436 107 113 Autoconf 2 471 435 32 4 SVG 4 386 378 4 4 C 3 153 89 47 17 Dockerfile 14 341 82 212 47 Batch 1 21 2 17 2 BASH 2 7 1 4 2 Plain Text 16 243 0 199 44 ------------------------------------------------------------------------------- Markdown 186 66853 0 53293 13560 |- BASH 6 86 58 20 8 |- Go 3 24 15 5 4 |- JSON 1 22 22 0 0 |- Shell 2 198 152 13 33 |- YAML 7 62 62 0 0 (Total) 67245 309 53331 13605 =============================================================================== Total 10706 3181979 2573916 391785 216278 =============================================================================== "}),e.add({id:72,href:"/study-kubernetes/docs/advanced/eco/",title:"2.8 生态",section:"第二部分 进阶实战",content:" 生态 # "}),e.add({id:73,href:"/study-kubernetes/docs/advanced/feature/hpa/",title:"Horizontal Pod Autoscaler",section:"2.3 功能",content:` Horizontal Pod Autoscaler # Pod 水平自动伸缩
`}),e.add({id:74,href:"/study-kubernetes/docs/design/code/1.0.0/",title:"1.0.0",section:"3.7 源码分析",content:" Kubernetes 1.0.0 源码分析 # https://github.com/kubernetes/kubernetes/tree/v1.0.0\n代码行数 # cloc # AlDanial/cloc $ cloc --version 1.94 $ cloc . 6328 text files. 5766 unique files. 668 files ignored. 4 errors: Line count, exceeded timeout: ./pkg/ui/data/swagger/datafile.go Line count, exceeded timeout: ./third_party/swagger-ui/swagger-ui.js Line count, exceeded timeout: ./third_party/swagger-ui/swagger-ui.min.js Line count, exceeded timeout: ./third_party/ui/bower_components/angular/angular.min.js github.com/AlDanial/cloc v 1.94 T=28.96 s (199.1 files/s, 40061.4 lines/s) -------------------------------------------------------------------------------- Language files blank comment code -------------------------------------------------------------------------------- Go 3745 95221 115772 599870 JavaScript 443 17244 56651 92961 JSON 174 4 0 44954 Markdown 393 10130 0 30592 CSS 170 3226 1088 22298 Bourne Shell 211 3315 5908 14438 SVG 36 6 37 12614 HTML 84 1556 60 6340 YAML 222 492 1140 5434 Text 43 859 0 4619 Protocol Buffers 13 575 1178 1828 Bourne Again Shell 24 309 249 1613 Python 22 301 482 1087 SaltStack 36 124 117 983 LESS 7 166 54 912 make 53 267 285 602 Dockerfile 68 196 369 508 XML 9 0 0 213 C 2 27 13 175 Java 1 18 10 126 SCSS 2 29 0 124 CoffeeScript 1 12 3 108 Assembly 1 11 10 36 PHP 1 6 0 27 Properties 2 2 4 18 INI 2 1 0 9 TOML 1 0 0 3 -------------------------------------------------------------------------------- SUM: 5766 134097 183430 842492 -------------------------------------------------------------------------------- tokei # XAMPPRocky/tokei $ tokei -V tokei 12.0.4 compiled with serialization support: json, cbor, yaml $ tokei --sort code --exclude test =============================================================================== Language Files Lines Code Comments Blanks =============================================================================== Go 3765 803073 611213 104189 87671 JavaScript 411 153108 88958 50674 13476 JSON 154 44424 44420 0 4 CSS 173 26388 22087 1156 3145 Shell 208 23742 14575 5960 3207 SVG 58 12895 12847 41 7 YAML 195 6751 5180 1132 439 Protocol Buffers 13 3581 2220 786 575 BASH 21 2006 1475 245 286 Python 23 1886 1178 405 303 LESS 7 1132 912 54 166 Makefile 50 1092 559 285 248 Dockerfile 66 1051 491 369 191 C 2 215 175 13 27 XML 4 140 140 0 0 Java 1 154 126 10 18 Sass 3 153 124 0 29 Autoconf 2 111 109 2 0 CoffeeScript 1 123 108 3 12 Assembly 1 57 36 10 11 PHP 1 33 27 0 6 INI 2 10 9 0 1 TOML 1 3 3 0 0 Plain Text 44 5486 0 4626 860 ------------------------------------------------------------------------------- HTML 96 6606 5616 67 923 |- CSS 12 20 20 0 0 |- JavaScript 16 158 122 15 21 (Total) 6784 5758 82 944 ------------------------------------------------------------------------------- Markdown 396 35167 0 25424 9743 |- BASH 27 620 573 24 23 |- C 1 46 29 15 2 |- Go 42 1941 1381 285 275 |- HTML 7 99 67 8 24 |- INI 1 10 6 2 2 |- JavaScript 5 52 52 0 0 |- JSON 13 741 729 0 12 |- PHP 1 33 27 0 6 |- Python 1 12 7 1 4 |- Shell 36 828 791 7 30 |- YAML 23 1064 1024 40 0 (Total) 40613 4686 25806 10121 =============================================================================== Total 5698 1135011 817416 195848 121747 =============================================================================== "}),e.add({id:75,href:"/study-kubernetes/docs/design/code/",title:"3.7 源码分析",section:"第三部分 设计与实现",content:` 源码分析 # Kubernetes 要求的 Go 版本 # Kubernetes requires Go 1.0 - 1.2 1.4.2 1.3, 1.4 1.6 1.5, 1.6 1.7 - 1.7.5 1.7 1.8.1 1.8 1.8.3 1.9 1.9.1 1.10 1.9.1 1.11 1.10.2 1.12 1.10.4 1.13 1.11.13 1.14 - 1.16 1.12.9 1.17 - 1.18 1.13.15 1.19 - 1.20 1.15.5 1.21 - 1.22 1.16.7 1.23 1.17 1.24+ 1.18 参考：
Kubernetes Development Guide `}),e.add({id:76,href:"/study-kubernetes/docs/advanced/network/",title:"Kubernetes 网络",section:"第二部分 进阶实战",content:" Kubernetes 网络 # "}),e.add({id:77,href:"/study-kubernetes/docs/advanced/eco/docker/tutorial/",title:"教程",section:"Docker",content:` Docker 教程 # p8952/bocker # Docker implemented in around 100 lines of bash
yeasy/docker_practice # Learn and understand Docker technologies, with real DevOps practice!
Docker 从入门到实践
play-with-docker/play-with-docker # Play With Docker gives you the experience of having a free Alpine Linux Virtual Machine in the cloud where you can build and run Docker containers and even create clusters with Docker features like Swarm Mode.
Under the hood DIND or Docker-in-Docker is used to give the effect of multiple VMs/PCs.
A live version is available at: http://play-with-docker.com/
`}),e.add({id:78,href:"/study-kubernetes/docs/advanced/eco/docker/attention/",title:"关注项目",section:"Docker",content:` 关注 # moby/moby # Moby Project - a collaborative project for the container ecosystem to assemble container-based systems https://mobyproject.org/
docker/docker-ce # Docker CE https://www.docker.com/community-edition
play-with-docker/play-with-docker # Play With Docker gives you the experience of having a free Alpine Linux Virtual Machine in the cloud where you can build and run Docker containers and even create clusters with Docker features like Swarm Mode.
Under the hood DIND or Docker-in-Docker is used to give the effect of multiple VMs/PCs.
A live version is available at: http://play-with-docker.com/
gliderlabs/logspout # Log routing for Docker container logs
`})})()