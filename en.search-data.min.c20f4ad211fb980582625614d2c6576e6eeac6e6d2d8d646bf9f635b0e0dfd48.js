'use strict';(function(){const b={cache:!0};b.doc={id:'id',field:['title','content'],store:['title','href','section']};const a=FlexSearch.create('balance',b);window.bookSearchIndex=a,a.add({id:0,href:'/study-kubernetes/docs/advanced/eco/docker/image/alpine/',title:"alpine",section:"镜像",content:"alpine #  安装 telnet #  apk add busybox-extras\n"}),a.add({id:1,href:'/study-kubernetes/docs/basic/arch/component/api-server/',title:"api-server",section:"组件",content:"api-server 基础 #  "}),a.add({id:2,href:'/study-kubernetes/docs/advanced/eco/docker/quick/',title:"Docker 快速上手",section:"Docker",content:"Docker 快速上手 #  安装 #  MacOS 下载: https://download.docker.com/mac/stable/Docker.dmgLinux Windows  镜像加速 #  阿里云 获取你的加速地址：https://cr.console.aliyun.com/undefined/instances/mirrors\n针对安装了 Docker for Mac 的用户，您可以参考以下配置步骤： 右键点击桌面顶栏的 docker 图标，选择 Preferences ，在 Daemon 标签（Docker 17.03 之前版本为 Advanced 标签）下的 Registry mirrors 列表中将 https://pjuig8sx.mirror.aliyuncs.com 加到 \u0026ldquo;registry-mirrors\u0026rdquo; 的数组里，点击 Apply \u0026amp; Restart 按钮，等待 Docker 重启并应用配置的镜像加速器。\nvi ~/.docker/deamon.json\n{ \u0026#34;experimental\u0026#34;: false, \u0026#34;debug\u0026#34;: true, \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://pjuig8sx.mirror.aliyuncs.com\u0026#34;, \u0026#34;https://mirror.baidubce.com\u0026#34;, \u0026#34;https://hub-mirror.c.163.com\u0026#34; ] } "}),a.add({id:3,href:'/study-kubernetes/docs/basic/arch/network/port/',title:"K8s 端口",section:"K8s 网络",content:"K8s 端口 #   service 的端口  port：负责处理对内的通信，  访问方式：clusterIP:port 或者 externalIP:port  externalIP 不归 kubernetes 管，cluster administrator 自己负责     nodePort：在 node 上，负责对外通信  访问方式：NodeIP:NodePort     pod 的端口  targetPort：在 pod 上  从 port 和 nodePort 上来的流量，经过 kube-proxy 流入到后端 pod 的 targetPort 上，最后进入容器     容器的端口  containerPort：在容器上，用于被 pod 绑定  targetPort 和 containerPort 是一致的 是可选的，仅仅是提示信息 容器中任何监听 0.0.0.0 的端口，都会暴露出来 无法被更新 主机的端口   hostPort：容器暴露的端口映射到的主机端口  尽量不要为 Pod 指定 hostPort 将 Pod 绑定到 hostPort 时，它会限制 Pod 可以调度的位置数，因为每个 \u0026lt;hostIP, hostPort, protocol\u0026gt; 组合必须是唯一的 如果您没有明确指定 hostIP 和 protocol，Kubernetes 将使用 0.0.0.0 作为默认 hostIP 和 TCP 作为默认 protocol      pod 模板指定端口 #  作用类似于 docker -p 选项\n containerPort: 容器需要暴露的端口 hostPort: 容器暴露的端口映射到的主机端口   service 指定端口 #  service (ClusterIP) #   port: service 中 clusterIP 对应的端口 targetPort: clusterIP 作为负载均衡，后端目标实例 (容器) 的端口  service (NodePort) #   nodePort: cluster ip 只能集群内部访问  范围是 30000-32767，这个值在 API server 的配置文件中，用 --service-node-port-range 定义 源与目标需要满足两个条件:  kube-proxy 正常运行 跨主机容器网络通信正常   nodePort 会在每个 kubelet 节点的宿主机开启一个端口，用于应用集群外部访问 nodePort 到 clusterIP 的映射，是 kube-proxy 实现的。     参考 #   配置最佳实践 从外部访问 Kubernetes 中的 Pod  关于在 kubenretes 中暴露 Pod 及服务的 5 种方式    "}),a.add({id:4,href:'/study-kubernetes/docs/basic/concept/resource/pod/',title:"Pod",section:"API 对象",content:"Pod #  操作 #  删除被驱逐的 pod\nkubectl get pods | grep Evicted | awk \u0026#39;{print $1}\u0026#39; | xargs kubectl delete pod Infra 容器 #  使用镜像：k8s.gcr.io/pause\n这个镜像是一个用汇编语言编写的、永远处于 “暂停” 状态的容器，解压后的大小也只有 100~200 KB 左右。\nInit 容器 #  以 Init: 开始的 Pod 状态概括表示 Init 容器的执行状态。\n下表展示了在调试 Init 容器时可能见到的状态值。\n   状态 含义     Init:N/M Pod 中有 M 个 Init 容器，其中 M 已经完成   Init:Error Init 容器执行错误   Init:CrashLoopBackOff Init 容器已经失败多次   Pending Pod 还没有开始执行 Init 容器   PodInitializing or Running Pod 已经完成执行 Init 容器    如果一个 Pod 停滞在 Pending 状态，表示 Pod 没有被调度到节点上。通常这是因为 某种类型的资源不足导致无法调度。 查看上面的 kubectl describe \u0026hellip; 命令的输出，其中应该显示了为什么没被调度的原因。\n常见原因:\n 资源不足 使用了 hostPort  参考：\n 应用故障排查  PodStatus #  type PodStatus struct { // The phase of a Pod is a simple, high-level summary of where the Pod is in its lifecycle. 	// The conditions array, the reason and message fields, and the individual container status 	// arrays contain more detail about the pod\u0026#39;s status. 	// There are five possible phase values: 	// 	// Pending: The pod has been accepted by the Kubernetes system, but one or more of the 	// container images has not been created. This includes time before being scheduled as 	// well as time spent downloading images over the network, which could take a while. 	// Running: The pod has been bound to a node, and all of the containers have been created. 	// At least one container is still running, or is in the process of starting or restarting. 	// Succeeded: All containers in the pod have terminated in success, and will not be restarted. 	// Failed: All containers in the pod have terminated, and at least one container has 	// terminated in failure. The container either exited with non-zero status or was terminated 	// by the system. 	// Unknown: For some reason the state of the pod could not be obtained, typically due to an 	// error in communicating with the host of the pod. 	// 	// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#pod-phase 	// +optional 	Phase PodPhase `json:\u0026#34;phase,omitempty\u0026#34; protobuf:\u0026#34;bytes,1,opt,name=phase,casttype=PodPhase\u0026#34;` // Current service state of pod. 	// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#pod-conditions 	// +optional 	// +patchMergeKey=type 	// +patchStrategy=merge 	Conditions []PodCondition `json:\u0026#34;conditions,omitempty\u0026#34; patchStrategy:\u0026#34;merge\u0026#34; patchMergeKey:\u0026#34;type\u0026#34; protobuf:\u0026#34;bytes,2,rep,name=conditions\u0026#34;` // A human readable message indicating details about why the pod is in this condition. 	// +optional 	Message string `json:\u0026#34;message,omitempty\u0026#34; protobuf:\u0026#34;bytes,3,opt,name=message\u0026#34;` // A brief CamelCase message indicating details about why the pod is in this state. 	// e.g. \u0026#39;Evicted\u0026#39; 	// +optional 	Reason string `json:\u0026#34;reason,omitempty\u0026#34; protobuf:\u0026#34;bytes,4,opt,name=reason\u0026#34;` // nominatedNodeName is set only when this pod preempts other pods on the node, but it cannot be 	// scheduled right away as preemption victims receive their graceful termination periods. 	// This field does not guarantee that the pod will be scheduled on this node. Scheduler may decide 	// to place the pod elsewhere if other nodes become available sooner. Scheduler may also decide to 	// give the resources on this node to a higher priority pod that is created after preemption. 	// As a result, this field may be different than PodSpec.nodeName when the pod is 	// scheduled. 	// +optional 	NominatedNodeName string `json:\u0026#34;nominatedNodeName,omitempty\u0026#34; protobuf:\u0026#34;bytes,11,opt,name=nominatedNodeName\u0026#34;` // IP address of the host to which the pod is assigned. Empty if not yet scheduled. 	// +optional 	HostIP string `json:\u0026#34;hostIP,omitempty\u0026#34; protobuf:\u0026#34;bytes,5,opt,name=hostIP\u0026#34;` // IP address allocated to the pod. Routable at least within the cluster. 	// Empty if not yet allocated. 	// +optional 	PodIP string `json:\u0026#34;podIP,omitempty\u0026#34; protobuf:\u0026#34;bytes,6,opt,name=podIP\u0026#34;` // RFC 3339 date and time at which the object was acknowledged by the Kubelet. 	// This is before the Kubelet pulled the container image(s) for the pod. 	// +optional 	StartTime *metav1.Time `json:\u0026#34;startTime,omitempty\u0026#34; protobuf:\u0026#34;bytes,7,opt,name=startTime\u0026#34;` // The list has one entry per init container in the manifest. The most recent successful 	// init container will have ready = true, the most recently started container will have 	// startTime set. 	// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#pod-and-container-status 	InitContainerStatuses []ContainerStatus `json:\u0026#34;initContainerStatuses,omitempty\u0026#34; protobuf:\u0026#34;bytes,10,rep,name=initContainerStatuses\u0026#34;` // The list has one entry per container in the manifest. Each entry is currently the output 	// of `docker inspect`. 	// More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#pod-and-container-status 	// +optional 	ContainerStatuses []ContainerStatus `json:\u0026#34;containerStatuses,omitempty\u0026#34; protobuf:\u0026#34;bytes,8,rep,name=containerStatuses\u0026#34;` // The Quality of Service (QOS) classification assigned to the pod based on resource requirements 	// See PodQOSClass type for available QOS classes 	// More info: https://git.k8s.io/community/contributors/design-proposals/node/resource-qos.md 	// +optional 	QOSClass PodQOSClass `json:\u0026#34;qosClass,omitempty\u0026#34; protobuf:\u0026#34;bytes,9,rep,name=qosClass\u0026#34;` } Pod phase（运行阶段） #  Pod 的 status 定义在 PodStatus 对象中，其中有一个 Phase（运行阶段） 字段。\n下面是 phase 可能的值：\n   状态 涵义 备注     Pending Pod 被系统接受，至少有一个容器未被创建    Running Pod 已绑定 Node，所有容器已创建，至少一个容器在运行（或处于正在启动/重启）    Succeeded Pod 中所有容器被成功终止，不再重启    Failed Pod 中所有容器被终止，至少有一个容器是因为失败终止（非 0 退出或被系统终止）    Unknown 无法获取 Pod 状态（与 Pod 所在主机通信失败）     PodCondition #  Pod 有一个 PodStatus 对象，其中包含一个 PodCondition 数组。\nPodCondition 数组的每个元素都有一个 type 字段和一个 status 字段。\n type 字段是字符串，可能的值有  PodScheduled Ready Initialized Unschedulable   status 字段是一个字符串，可能的值有  True False Unknown    Terminating #  ContainerCreating #   资源限制 #  request #   requests 用于 schedule 阶段，在调度 pod 保证所有 pod 的 requests 总和小于 node 能提供的计算能力 requests.cpu 被转成 docker 的 --cpu-shares 参数，与 cgroup cpu.shares 功能相同  设置容器的 cpu 的相对权重 该参数在 CPU 资源不足时生效，根据容器 requests.cpu 的比例来分配 cpu 资源 CPU 资源充足时，requests.cpu 不会限制 container 占用的最大值，container 可以独占 CPU   requests.memory 没有对应的 docker 参数，作为 k8s 调度依据 使用 requests 来设置各容器需要的最小资源  limit #   limits 限制运行时容器占用的资源 limits.cpu 会被转换成 docker 的\u0026ndash;cpu-quota 参数。与 cgroup cpu.cfs_quota_us 功能相同  限制容器的最大 CPU 使用率。 cpu.cfs_quota_us 参数与 cpu.cfs_period_us 结合使用，后者设置时间周期 k8s 将 docker 的\u0026ndash;cpu-period 参数设置 100 毫秒。对应着 cgroup 的 cpu.cfs_period_us limits.cpu 的单位使用 m，千分之一核   limits.memory 会被转换成 docker 的\u0026ndash;memory 参数。用来限制容器使用的最大内存 当容器申请内存超过 limits 时会被终止   问答 #  为什么要有 pod #   参考 #  "}),a.add({id:5,href:'/study-kubernetes/docs/basic/concept/resource/replicaset/',title:"replicaset",section:"API 对象",content:"replicaset #  问答 #  为什么要有 replicaset ？ #  "}),a.add({id:6,href:'/study-kubernetes/docs/basic/install/',title:"安装",section:"第一部分 基础入门",content:"Kubernetes 安装 #  安装方式对比 #  MacOS Minikube  Pros:  Mature solution Works on Windows (any version and edition), Mac and Linux Multiple drivers that can match any environment Installs several plugins (such as dashboard) by default Very flexible on installation requirements and upgrades  Cons:  Installation and removal not as streamlined as other solutions Does not integrate into the MacOS UI  Docker Desktop Pros:  Very easy installation for beginners All-in-one Docker and Kubernetes solution Configurable via UI  Cons:  Relatively new, possibly unstable Limited configuration options (i.e. driver support)  参考  Local Kubernetes for Mac– MiniKube vs Docker Desktop  Linux Minikube  Pros:  Mature solution Works on Windows (any version and edition), Mac and Linux Multiple drivers that can match any environment Can work with or without an intermediate VM on Linux (vmdriver=none) Installs several plugins (such as dashboard) by default Very flexible on installation requirements and upgrades  Cons:  Installation and removal not as streamlined as other solutions Can conflict with local installation of other tools (such as Virtualbox)  MicroK8s  Pros:  Very easy to install, upgrade, remove Completely isolated from other tools in your machine Does not need a VM, all services run locally  Cons:  Only available for Snap supported Linux Distributions Relatively new, possible unstable Minikube can also run directly on Linux (vmdriver=none), so MicroK8s value proposition is diminished  参考  Local Kubernetes for Linux – MiniKube vs MicroK8s  Windows Minikube  Pros:  Mature solution Works on Windows (any version and edition), Mac and Linux Multiple drivers that can match any environment Can work with or without an intermediate VM on Linux (vmdriver=none) Installs several plugins (such as dashboard) by default Very flexible on installation requirements and upgrades  Cons:  Installation and removal not as streamlined as other solutions Can conflict with local installation of other tools (such as Virtualbox)  Docker Desktop Pros:  Very easy installation for beginners The best solution for running Windows containers Integrated Docker and Kubernetes solution  Cons:  Requires Windows 10 Pro edition and Hyper V Cannot use simultaneously with Virtualbox, Vagrant etc Relatively new, possibly unstable The sole solution for running Windows containers  参考  Local Kubernetes for Windows – MiniKube vs Docker Desktop   kind #  kubernetes-sigs/kind Kubernetes IN Docker - local clusters for testing Kubernetes\n 教程 #  和我一步步部署 kubernetes 集群 #  opsnull/follow-me-install-kubernetes-cluster 本系列文档介绍使用二进制部署 kubernetes v1.16.6 集群的所有步骤（Hard-Way 模式）。\n在部署的过程中，将详细列出各组件的启动参数，它们的含义和可能遇到的问题。\n"}),a.add({id:7,href:'/study-kubernetes/docs/basic/concept/resource/configmap/',title:"ConfigMap",section:"API 对象",content:"ConfigMap 基础 #  ConfigMap 在运行时会将配置文件、命令行参数、环境变量、端口号以及其他配置工件绑定到 Pod 的容器和系统组件。借助 ConfigMap，您可以将配置与 Pod 和组件分开，这有助于保持工作负载的可移植性，使其配置更易于更改和管理，并防止将配置数据硬编码到 Pod 规范。\nConfigMap 可用于存储和共享非敏感、未加密的配置信息。要在集群中使用敏感信息，您必须使用 Secret。\n创建 ConfigMap #  使用以下命令创建 ConfigMap：\nkubectl create configmap [NAME] [DATA]\n[DATA] 可以是： 包含一个或多个配置文件的目录的路径，使用 \u0026ndash;from-file 标志指示 键值对，每个键值对都使用 \u0026ndash;from-literal 标志指定 如需详细了解 kubectl create，请参阅参考文档。\n您还可以通过在 YAML 清单文件中定义 ConfigMap 对象并使用 kubectl create -f [FILE] 部署对象来创建 ConfigMap。\n使用 ConfigMap #  apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: k8s.gcr.io/busybox command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)\u0026#34;] env: - name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: name: special-config key: SPECIAL_LEVEL - name: SPECIAL_TYPE_KEY valueFrom: configMapKeyRef: name: special-config key: SPECIAL_TYPE restartPolicy: Never "}),a.add({id:8,href:'/study-kubernetes/docs/basic/concept/resource/deployment/',title:"Deployment",section:"API 对象",content:"Deployment #  故障排查 #  下载 pdf 版本\n参考：\n A visual guide on troubleshooting Kubernetes deployments  Kubernetes Deployment 故障排查常见方法     问答 #  为什么要有 Deployment #  "}),a.add({id:9,href:'/study-kubernetes/docs/basic/arch/component/schedule/',title:"Schedule",section:"组件",content:"Schedule 基础 #  "}),a.add({id:10,href:'/study-kubernetes/docs/basic/arch/component/controller-manager/',title:"Controller Manager",section:"组件",content:"Controller Manager 基础 #  "}),a.add({id:11,href:'/study-kubernetes/docs/basic/concept/resource/crd/',title:"CRD",section:"API 对象",content:"CRD #  如何实现一个 CRD ？ #   关注 #  GoogleCloudPlatform/metacontroller #  Metacontroller is an add-on for Kubernetes that makes it easy to write and deploy custom controllers in the form of simple scripts.\nThis is not an officially supported Google product. Although this open-source project was started by GKE, the add-on works the same in any Kubernetes cluster.\n"}),a.add({id:12,href:'/study-kubernetes/docs/basic/install/kubeadm/',title:"Kubeadm",section:"安装",content:"Kubeadm #  kubernetes/kubeadm Kubeadm is a tool built to provide best-practice \u0026ldquo;fast paths\u0026rdquo; for creating Kubernetes clusters. It performs the actions necessary to get a minimum viable, secure cluster up and running in a user friendly way. Kubeadm\u0026rsquo;s scope is limited to the local node filesystem and the Kubernetes API, and it is intended to be a composable building block of higher level tools.\n教程 #  Getting Started With Kubeadm #  from katacoda\n参考 #  https://www.katacoda.com #  Katacoda - Interactive Learning Platform for Software Engineers\n"}),a.add({id:13,href:'/study-kubernetes/docs/basic/install/microk8s/',title:"MicroK8s",section:"安装",content:"MicroK8s #  ubuntu/microk8s MicroK8s is a small, fast, single-package Kubernetes for developers, IoT and edge.\nhttps://microk8s.io\nPros: #   Very easy to install, upgrade, remove Completely isolated from other tools in your machine Does not need a VM, all services run locally  Cons: #   Only available for Snap supported Linux Distributions Relatively new, possible unstable Minikube can also run directly on Linux (vmdriver=none), so MicroK8s value proposition is diminished   参考 #   Local Kubernetes for Linux – MiniKube vs MicroK8s  "}),a.add({id:14,href:'/study-kubernetes/docs/code/',title:"第三部分 设计与实现",section:"Docs",content:"如无特殊说明，源码版本为 1.18.2\nv1.18 Release Notes\n "}),a.add({id:15,href:'/study-kubernetes/docs/basic/concept/resource/',title:"API 对象",section:"1.2 概念",content:"API 对象 #  NAME SHORTNAMES APIGROUP NAMESPACED KIND bindings true Binding componentstatuses cs false ComponentStatus configmaps cm true ConfigMap endpoints ep true Endpoints events ev true Event limitranges limits true LimitRange namespaces ns false Namespace nodes no false Node persistentvolumeclaims pvc true PersistentVolumeClaim persistentvolumes pv false PersistentVolume pods po true Pod podtemplates true PodTemplate replicationcontrollers rc true ReplicationController resourcequotas quota true ResourceQuota secrets true Secret serviceaccounts sa true ServiceAccount services svc true Service initializerconfigurations admissionregistration.k8s.io false InitializerConfiguration mutatingwebhookconfigurations admissionregistration.k8s.io false MutatingWebhookConfiguration validatingwebhookconfigurations admissionregistration.k8s.io false ValidatingWebhookConfiguration customresourcedefinitions crd,crds apiextensions.k8s.io false CustomResourceDefinition apiservices apiregistration.k8s.io false APIService controllerrevisions apps true ControllerRevision daemonsets ds apps true DaemonSet deployments deploy apps true Deployment replicasets rs apps true ReplicaSet statefulsets sts apps true StatefulSet auditsinks auditregistration.k8s.io false AuditSink tokenreviews authentication.k8s.io false TokenReview localsubjectaccessreviews authorization.k8s.io true LocalSubjectAccessReview selfsubjectaccessreviews authorization.k8s.io false SelfSubjectAccessReview selfsubjectrulesreviews authorization.k8s.io false SelfSubjectRulesReview subjectaccessreviews authorization.k8s.io false SubjectAccessReview horizontalpodautoscalers hpa autoscaling true HorizontalPodAutoscaler cronjobs cj batch true CronJob jobs batch true Job certificatesigningrequests csr certificates.k8s.io false CertificateSigningRequest adapters config.istio.io true adapter attributemanifests config.istio.io true attributemanifest handlers config.istio.io true handler httpapispecbindings config.istio.io true HTTPAPISpecBinding httpapispecs config.istio.io true HTTPAPISpec instances config.istio.io true instance quotaspecbindings config.istio.io true QuotaSpecBinding quotaspecs config.istio.io true QuotaSpec rules config.istio.io true rule templates config.istio.io true template leases coordination.k8s.io true Lease events ev events.k8s.io true Event daemonsets ds extensions true DaemonSet deployments deploy extensions true Deployment ingresses ing extensions true Ingress networkpolicies netpol extensions true NetworkPolicy podsecuritypolicies psp extensions false PodSecurityPolicy replicasets rs extensions true ReplicaSet istiooperators iop install.istio.io true IstioOperator destinationrules dr networking.istio.io true DestinationRule envoyfilters networking.istio.io true EnvoyFilter gateways gw networking.istio.io true Gateway serviceentries se networking.istio.io true ServiceEntry sidecars networking.istio.io true Sidecar virtualservices vs networking.istio.io true VirtualService workloadentries we networking.istio.io true WorkloadEntry networkpolicies netpol networking.k8s.io true NetworkPolicy poddisruptionbudgets pdb policy true PodDisruptionBudget podsecuritypolicies psp policy false PodSecurityPolicy clusterrolebindings rbac.authorization.k8s.io false ClusterRoleBinding clusterroles rbac.authorization.k8s.io false ClusterRole rolebindings rbac.authorization.k8s.io true RoleBinding roles rbac.authorization.k8s.io true Role clusterrbacconfigs rbac.istio.io false ClusterRbacConfig rbacconfigs rbac.istio.io true RbacConfig servicerolebindings rbac.istio.io true ServiceRoleBinding serviceroles rbac.istio.io true ServiceRole priorityclasses pc scheduling.k8s.io false PriorityClass authorizationpolicies security.istio.io true AuthorizationPolicy peerauthentications pa security.istio.io true PeerAuthentication requestauthentications ra security.istio.io true RequestAuthentication podpresets settings.k8s.io true PodPreset storageclasses sc storage.k8s.io false StorageClass volumeattachments storage.k8s.io false VolumeAttachment 声明式 API #   提交一个定义好的 API 对象来 “声明” 启动一个对应的控制器进行调谐 / 编排  控制器 #  一个 Kubernetes 的控制器，实际上就是一个死循环：\n 不断地获取 “实际状态”， 然后与 “期望状态” 作对比， 并以此为依据决定下一步的操作  workload #  Kubernetes divides workloads into different types. The most popular types supported by Kubernetes are:\n Deployments StatefulSets DaemonSets Jobs CronJobs  参考：\n Kubernetes Workloads and Pods   API Resource #  Deployment #   metadata  name   spec  replicas selector  matchLabels  app version     template  metadata  annotations labels  app version     spec  serviceAccountName containers  name image imagePullPolicy ports  name containerPort            Service #   metadata  name labels  app     spec  ports  name port targetPort   selector  app      CustomResourceDefinition #   metadata  name   spec  group version names  kind plural   scope    "}),a.add({id:16,href:'/study-kubernetes/docs/basic/arch/component/kube-proxy/',title:"Kube-proxy",section:"组件",content:"Kube-proxy #  "}),a.add({id:17,href:'/study-kubernetes/docs/basic/arch/component/kubelet/',title:"Kubelet",section:"组件",content:"Kubelet #    kubelet 默认最多运行 110 个 pods\n$ kubectl describe node xxxx | grep -A 7 'Capacity' Capacity: cpu: 4 ephemeral-storage: 41152716Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 16431024Ki pods: 110  kubelet 的 --max-pods 选项可以指定运行的最大 Pod 数目 因为 flanneld 配置的本节点 Pod 网段是 /24，所以一个 Node 最多运行 254 个 Pod（flannel、docker0 占用 2 个），--max-pods 不能超过该值；    Kubelet 创建容器进程 #   CRI: Container Runtime Interface CNI: Container Network Interface CSI: Container Storage Interface OCI: Open Container Initiative  Docker #  kubelet -\u0026gt; dockershim -\u0026gt; docker daemon -\u0026gt; containerd -\u0026gt; containerd-shim -\u0026gt; runc -\u0026gt; container\n参考：\n 白话 Kubernetes Runtime containerd, containerd-shim 和 runc 的依存关系  containerd/cri #  kubelet -\u0026gt; cri plugin -\u0026gt; containerd -\u0026gt; containerd-shim -\u0026gt; runc -\u0026gt; container\nKubernetes v1.20 弃用 docker-shim #  "}),a.add({id:18,href:'/study-kubernetes/docs/basic/arch/network/dns/',title:"K8s DNS",section:"K8s 网络",content:"K8s DNS #   DNS 策略 #   Default  The Pod inherits the name resolution configuration from the node that the pods run on   ClusterFirst  Any DNS query that does not match the configured cluster domain suffix, such as “www.kubernetes.io”, is forwarded to the upstream nameserver inherited from the node.   ClusterFirstWithHostNet  For Pods running with hostNetwork, you should explicitly set its DNS policy “ClusterFirstWithHostNet”   None  It allows a Pod to ignore DNS settings from the Kubernetes environment. All DNS settings are supposed to be provided using the dnsConfig field in the Pod Spec.    “Default” is not the default DNS policy.\nIf dnsPolicy is not explicitly specified, then “ClusterFirst” is used.\n 参考：\n Pod\u0026rsquo;s DNS Policy   k8s 中域名是如何被解析的 #  在 k8s 中，一个 Pod 如果要访问相同同 Namespace 下的 Service（比如 user-svc），那么只需要 curl user-svc。 如果 Pod 和 Service 不在同一域名下，那么就需要在 Service Name 之后添加上 Service 所在的 Namespace（比如 beta），curl user-svc.beta。 那么 k8s 是如何知道这些域名是内部域名并为他们做解析的呢？\n无论是在 宿主机 或者是在 k8s 集群中，DNS 解析会依赖这个三个文件\n /etc/host.conf /etc/hosts /etc/resolv.conf  /etc/resolv.conf #  resolv.conf 是 Pod 在 dnsPolicy: ClusterFirst 的情况下，k8s 为其自动生成的。 在该 Pod 内请求的所有的域名解析都需要经过 DNS Service 进行解析，不管是集群内部域名还是外部域名。\n每行都会以一个关键字开头，然后跟配置参数。\n在集群中主要使用到的关键词有 3 个\n nameserver 定义 DNS 服务器的 IP 地址（Kube-DNS 的 Service IP） search 定义域名的搜索列表，当查询的域名中包含的 . 的数量少于 options.ndots 的值时，会依次匹配列表中的每个值 options 定义域名查找时的配置信息  nameserver、search 和 options 都是可以通过 dnsConfig 字段进行配置的，详细参考官方文档\n例如\nnameserver 10.250.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5 nameserver #  nameserver 所对应的地址正是 DNS Service 的 Cluster IP（该值在启动 kubelet 的时候，通过 clusterDNS 指定）。\nsearch 域 #  search 域默认包含了 namespace.svc.cluster.local、svc.cluster.local 和 cluster.local 三种。\n当我们在 Pod 中访问 a Service 时（ curl a ），会选择 nameserver 10.250.0.10 进行解析，然后依次带入 search 域进行 DNS 查找，直到找到为止。\n$ curl a a.default.svc.cluster.local 显然因为 Pod 和 a Service 在同一 Namespace 下，所以第一次 lookup 就能找到。\n如果 Pod 要访问不同 Namespace（例如： beta ）下的 Service b （ curl b.beta ），会经过两次 DNS 查找，分别是\n$ curl b.beta b.beta.default.svc.cluster.local # Not Found b.beta.svc.cluster.local # Found 正是因为 search 的顺序性，所以访问同一 Namespace 下的 Service， curl a 是要比 curl a.default 的效率更高的，因为后者多经过了一次 DNS 解析。\n$ curl a a.default.svc.cluster.local # Found $ curl a.default b.default.default.svc.cluster.local # Not Found b.default.svc.cluster.local # Found options #  ndots #  ndots:5，表示：\n 如果需要 lookup 的 Domain 中包含少于 5 个 . ，那么将会被当做非绝对域名， 如果需要查询的 Domain 中包含大于或等于 5 个 . ，那么就会被当做绝对域名。  如果是绝对域名则不会走 search 域，\n如果是非绝对域名，就会按照 search 域中进行逐一匹配查询， 如果 search 走完了都没有找到，那么就会使用原域名进行查找。\n优化外网域名解析 #  在真正解析 http://iftech.io 之前，经历了\niftech.io.default.svc.cluster.local -\u0026gt; iftech.io.svc.cluster.local -\u0026gt; iftech.io.cluster.local -\u0026gt; iftech.io 这样也就意味着前 3 次 DNS 请求是浪费的，没有意义的。\n直接使用绝对域名 #  这是最简单直接的优化方式，可以直接在要访问的域名后面加上 . 如：iftech.io. ，这样就可以避免走 search 域进行匹配。\n配置 ndots #  比如配置 ndots:1，iftech.io. 就会使用原域名进行查找。\n CoreDNS vs KubeDNS #  在 kube-dns 中，一个 pod 内使用了数个容器：kubedns、dnsmasq 和 sidecar。\n kubedns 容器监视 Kubernetes API 并基于 Kubernetes DNS 规范提供 DNS 记录， dnsmasq 提供缓存和存根域支持， sidecar 提供指标和健康检查。  此设置会导致一些问题随着时间的推移而出现。首先，dnsmasq 中的安全漏洞导致过去需要发布 Kubernetes 安全补丁。 此外，由于 dnsmasq 处理存根域，但 kubedns 处理 External Services，因此你无法在外部服务中使用存根域，这非常限制该功能（参阅 dns＃131）。\n在 CoreDNS 中，所有这些功能都在一个容器中完成 —— 该容器运行用 Go 编写的进程。 启用的不同插件来复制（并增强）kube-dns 中的功能。\n为什么 pod 是 coredns，service 是 kube-dns？ #  其实是 CoreDNS\nCoreDNS is default from K8S 1.11. For previous installations it\u0026rsquo;s kube-dns.\n看 image，其他都是 metadata，不重要\n$ k describe pod coredns-6967fb4995-76trs -n kube-system | grep -i \u0026#34;image\u0026#34; Image: registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.3.1 问题详情\n$ k cluster-info Kubernetes master is running at https://192.168.99.102:8443 KubeDNS is running at https://192.168.99.102:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy $ k get po -n kube-system NAME READY STATUS RESTARTS AGE coredns-6967fb4995-76trs 1/1 Running 6 32d coredns-6967fb4995-grnzj 1/1 Running 6 32d etcd-minikube 1/1 Running 3 32d kube-addon-manager-minikube 1/1 Running 3 32d kube-apiserver-minikube 1/1 Running 3 32d kube-controller-manager-minikube 1/1 Running 3 32d kube-proxy-z765q 1/1 Running 3 32d kube-scheduler-minikube 1/1 Running 3 32d storage-provisioner 1/1 Running 5 32d $ k get svc -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 32d $ k describe svc/kube-dns -n kube-system Name: kube-dns Namespace: kube-system Labels: k8s-app=kube-dns kubernetes.io/cluster-service=true kubernetes.io/name=KubeDNS Annotations: prometheus.io/port: 9153 prometheus.io/scrape: true Selector: k8s-app=kube-dns Type: ClusterIP IP: 10.96.0.10 Port: dns 53/UDP TargetPort: 53/UDP Endpoints: 172.17.0.2:53,172.17.0.3:53 Port: dns-tcp 53/TCP TargetPort: 53/TCP Endpoints: 172.17.0.2:53,172.17.0.3:53 Port: metrics 9153/TCP TargetPort: 9153/TCP Endpoints: 172.17.0.2:9153,172.17.0.3:9153 Session Affinity: None Events: \u0026lt;none\u0026gt; 参考：\n Kubernetes: Kube-DNS vs CoreDNS  "}),a.add({id:19,href:'/study-kubernetes/docs/basic/arch/network/',title:"K8s 网络",section:"1.3 架构",content:"K8s 网络 #  "}),a.add({id:20,href:'/study-kubernetes/docs/basic/arch/network/troubleshoot/',title:"故障排查",section:"K8s 网络",content:"Kubernetes 网络故障排查 #  "}),a.add({id:21,href:'/study-kubernetes/docs/basic/quick/',title:"1.1 快速上手",section:"第一部分 基础入门",content:"Kubernetes 快速上手 #  ketacoda #  Kubernetes basics (by lizrice) #  In this scenario you\u0026rsquo;ll run your Go application in a Kubernetes cluster.\n"}),a.add({id:22,href:'/study-kubernetes/docs/code/component/',title:"3.1 Kubernetes 组件",section:"第三部分 设计与实现",content:"Kubernetes 组件 #  "}),a.add({id:23,href:'/study-kubernetes/docs/appendix/tutorial/',title:"4.1 教程",section:"第四部分 附录",content:"Kubernetes 教程 #  基础 #  collabnix/dockerlabs #  Docker - Beginners | Intermediate | Advanced https://dockerlabs.collabnix.com\nTo get started with Kubernetes, follow the below steps:\nOpen https://labs.play-with-kubernetes.com on your browser Click on Add Instances to setup first k8s node cluster\nkelseyhightower/kubernetes-the-hard-way #  Bootstrap Kubernetes the hard way on Google Cloud Platform. No scripts.\nfeiskyer/kubernetes-handbook #  Kubernetes Handbook （Kubernetes 指南） https://kubernetes.feisky.xyz\nplay-with-docker/play-with-docker #  Play With Docker gives you the experience of having a free Alpine Linux Virtual Machine in the cloud where you can build and run Docker containers and even create clusters with Docker features like Swarm Mode.\nUnder the hood DIND or Docker-in-Docker is used to give the effect of multiple VMs/PCs.\nA live version is available at: http://play-with-docker.com/\n进阶 #  daniel-hutao/k8s-source-code-analysis #  《k8s-1.13 版本源码分析》 https://hutao.tech/k8s-source-code-analysis/\n"}),a.add({id:24,href:'/study-kubernetes/docs/basic/arch/network/dns/coredns/',title:"CoreDNS",section:"K8s DNS",content:"CoreDNS #  coredns/coredns "}),a.add({id:25,href:'/study-kubernetes/docs/advanced/eco/docker/',title:"Docker",section:"2.8 生态",content:"Docker 基础 #  docker 资源 #   docker container ls：默认只列出正在运行的容器，-a 选项会列出包括停止的所有容器。 docker image ls：列出镜像信息，-a 选项会列出 intermediate 镜像 (就是其它镜像依赖的层)。 docker volume ls：列出数据卷。 docker network ls：列出 network。 docker info：显示系统级别的信息，比如容器和镜像的数量等。  清理 docker 资源 #  删除那些已停止的容器、dangling 镜像、未被容器引用的 network 和构建过程中的 cache\ndocker system prune # 默认不会删除那些未被任何容器引用的数据卷，需要使用 --volumes 进行删除 docker system prune --volumes # 直接删除，没有确认过程 docker system prune --all --force --volumes docker container prune # 删除所有退出状态的容器 docker volume prune # 删除未被使用的数据卷 docker image prune # 删除 dangling 或所有未被使用的镜像 docker container rm $(docker container ls -a -q) # 删除容器 docker image rm $(docker image ls -a -q) # 删除镜像 docker volume rm $(docker volume ls -q) # 删除数据卷 docker network rm $(docker network ls -q) # 删除 network # 列出所有容器 docker container ls -a -q # 停止所有容器 docker container stop $(docker container ls -a -q) # 删除所有资源 docker container stop $(docker container ls -a -q) \u0026amp;\u0026amp; docker system prune --all --force --volumns 参考：\n 如何快速清理 docker 资源   Docker 开源镜像 #  docker info 检测是否生效\nMacOS\nvi ~/.docker/deamon.json\n{ \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://pjuig8sx.mirror.aliyuncs.com\u0026#34;, \u0026#34;https://mirror.baidubce.com\u0026#34;, \u0026#34;https://hub-mirror.c.163.com\u0026#34; ] } https://hub-mirror.c.163.com/ # ustc 直接重定向到 163 了 # http://mirrors.ustc.edu.cn/help/dockerhub.html?highlight=docker https://docker.mirrors.ustc.edu.cn/ "}),a.add({id:26,href:'/study-kubernetes/docs/advanced/feature/operator/kubebuilder/',title:"kubebuilder",section:"Operator",content:"kubebuilder #  kubernetes-sigs/kubebuilder #  Kubebuilder - SDK for building Kubernetes APIs using CRDs http://book.kubebuilder.io/\n"}),a.add({id:27,href:'/study-kubernetes/docs/advanced/tool/kubectl/',title:"Kubectl",section:"2.2 工具",content:"Kubectl #  集群信息查询 #   kubectl cluster-info  node #   kubectl get nodes kubectl describe node   常用操作 #  设置 #  设置 role #  # 添加 kubectl label node ime-rd5-edge0-node2-kunlun kubernetes.io/role=master kubectl label node ime-rd5-edge0-node2-kunlun kubernetes.io/role=worker # 删除 kubectl label node ime-rd5-edge0-node2-kunlun kubernetes.io/role=worker # 重置 role kubectl label --overwrite nodes \u0026lt;your_node\u0026gt; kubernetes.io/role=\u0026lt;your_new_label\u0026gt; 获取 service ip, port #  kubectl get service/servicename -o jsonpath='{.spec.clusterIP}:{.spec.ports[*].port}'  kubectl get 结果排序 #  --sort-by= ##### pod # name kubectl --sort-by=.metadata.name get pod # status kubectl --sort-by=.status.phase get pod # restarts kubectl --sort-by=\u0026#39;.status.containerStatuses[0].restartCount\u0026#39; get pod # age kubectl --sort-by=.status.startTime get pod # ip kubectl --sort-by=.status.podIP get pod # node kubectl --sort-by=.spec.nodeName get pod ##### deployment # name kubectl --sort-by=.metadata.name get deployment # age kubectl --sort-by=.metadata.creationTimestamp get deployment # uptodate kubectl --sort-by=.status.updatedReplicas get deployment # available kubectl --sort-by=.metadata.availableReplicas get deployment # containers kubectl --sort-by=.spec.template.spec.containers[*].name get deployment # images kubectl --sort-by=.spec.template.spec.containers[*].image get deployment ##### service # name kubectl --sort-by=.metadata.name get service # age kubectl --sort-by=.metadata.creationTimestamp get service # type kubectl --sort-by=.spec.type get service # clusterip kubectl --sort-by=.spec.clusterIP get service # port kubectl --sort-by=.spec.ports[*].port get service 参考：\n kubectl Cheat Sheet  AATHITH/kubesort #  This a Bash Script that will help you forget the kubectl\u0026rsquo;s default, difficult to remember, sorting feature by making it simpler.\n 命令 #  autoscale #   参考 #   kubectl Cheat Sheet kubectl commands  "}),a.add({id:28,href:'/study-kubernetes/docs/basic/install/minikube/',title:"MiniKube",section:"安装",content:"MiniKube #  kubernetes/minikube Run Kubernetes locally\nhttps://minikube.sigs.k8s.io\n安装 #  MacOS 1. Install kubectl  2. Install a Hypervisor  HyperKit VirtualBox VMware Fusion   3. Install Minikube Homebrew brew install minikube\ncurl curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64 \\ \u0026amp;\u0026amp; chmod +x minikube sudo mv minikube /usr/local/bin Linux Windows  参考：https://kubernetes.io/docs/tasks/tools/install-minikube/\n启动 #    minikube start --registry-mirror=https://registry.docker-cn.com --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers\n  minikube start --cpus=2 --disk-size='10g' --image-mirror-country='cn' --image-repository='registry.cn-hangzhou.aliyuncs.com/google_containers'\n  minikube start --image-mirror-country='cn' --registry-mirror=https://registry.docker-cn.com   "}),a.add({id:29,href:'/study-kubernetes/docs/advanced/debug/pod/',title:"pod 排错",section:"2.5 故障排查",content:"pod 排错 #  排查 Pod 异常的常用命令如下：\n 查看 Pod 状态：kubectl get pods \u0026lt;pod-name\u0026gt; -n \u0026lt;namespace\u0026gt; -o wide 查看 Pod 的 yaml 配置：kubectl get pods \u0026lt;pod-name\u0026gt; -n \u0026lt;namespace\u0026gt; -o yaml 查看 Pod 的事件：kubectl describe pods \u0026lt;pod-name\u0026gt; -n \u0026lt;namespace\u0026gt; 查看 Pod 容器日志：kubectl logs -n \u0026lt;namespace\u0026gt; \u0026lt;pod-name\u0026gt; [-c \u0026lt;container-name\u0026gt;]  Pending 状态 #  Pending 状态说明 Pod 还没有调度到某个 Node 上面\n可以通过 kubectl describe pods \u0026lt;pod-name\u0026gt; -n \u0026lt;namespace\u0026gt; 命令查看到 Pod 的事件\n参考：\n Kubernetes 指南 - Pod 排错 排错指南 - Pod Kubernetes Docs - Troubleshoot Applications  "}),a.add({id:30,href:'/study-kubernetes/docs/basic/concept/resource/service/',title:"Service",section:"API 对象",content:"Service #  type #  ClusterIP #   通过集群的内部 IP 暴露服务，选择该值，服务只能够在集群内部可以访问 这也是默认的 ServiceType  NodePort #   通过每个 Node 上的 IP 和静态端口（NodePort）暴露服务 NodePort 服务会路由到 ClusterIP 服务，这个 ClusterIP 服务会自动创建 通过请求 \u0026lt;NodeIP\u0026gt;:\u0026lt;NodePort\u0026gt;，可以从集群的外部访问一个 NodePort 服务  LoadBalancer #   使用云提供商的负载局衡器，可以向外部暴露服务 外部的负载均衡器可以路由到 NodePort 服务和 ClusterIP 服务  访问方式 #   内部：ClusterIP:port (10.110.126.136:80) 外部：  hostIP:nodePort (机器 IP:30032) EXTERNAL-IP:port (xxxx:80)    # 没有 EXTERNAL-IP NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR istio-ingressgateway LoadBalancer 10.110.126.136 \u0026lt;pending\u0026gt; 15020:30332/TCP,80:30032/TCP,443:30551/TCP,31400:32430/TCP,15443:30294/TCP 13h app=istio-ingressgateway,istio=ingressgateway ExternalName #   通过返回 CNAME 和它的值，可以将服务映射到 externalName 字段的内容（例如， foo.bar.example.com） 没有任何类型代理被创建 CoreDNS 1.7 或更高版本才能使用 ExternalName 类型  "}),a.add({id:31,href:'/study-kubernetes/docs/advanced/eco/docker/command/',title:"命令",section:"Docker",content:"Docker 命令 #  docker ps #  docker ps vs docker container ls #  Management Commands vs Commands\nDocker 1.13+ introduced grouped commands to help organize a bunch of Docker commands. Both commands do the same thing.\nFor example docker container ls is the new way to do docker ps.\nSure it’s more typing, but it’s a lot more clear on what it does. Likewise, now you can run docker image ls, docker network ls or docker volume ls. There’s consistency across all of these commands.\n参考：\n Difference between docker ps vs docker container ls   docker attach vs docker exec #  $ sudo docker attach 665b4a1e17b6 #by ID or $ sudo docker attach loving_heisenberg #by Name $ root@665b4a1e17b6:/# $ sudo docker exec -i -t 665b4a1e17b6 /bin/bash #by ID or $ sudo docker exec -i -t loving_heisenberg /bin/bash #by Name $ root@665b4a1e17b6:/#  docker attach isn\u0026rsquo;t for running an extra thing in a container, it\u0026rsquo;s for attaching to the running process.  attach 只有一个实例   docker exec is specifically for running new things in a already started container, be it a shell or some other process.  exec 可以启动多个    参考：\n difference between docker attach and docker exec How to get bash or ssh into a running container in background mode?  "}),a.add({id:32,href:'/study-kubernetes/docs/advanced/eco/docker/version/',title:"版本",section:"Docker",content:"Docker 版本 #  查看版本 #  docker -v docker --version #查看版本 docker-compose --version #查看版本 docker-machine --version #查看版本 docker version #查看client和server端版本，并可以查看是否开启体验功能  历史版本 #  最新版本\n19.03 (2019-07-22) #  19.03.12 (2020-06-18) #   18.09 (2018-11-08) #  Docker 合并版本号 #  New in 18.09 is an aligned release model for Docker Engine - Community and Docker Engine - Enterprise. The new versioning scheme is YY.MM.x where x is an incrementing patch version. The enterprise engine is a superset of the community engine. They will ship concurrently with the same x patch version based on the same code base.\n分拆 client 与 daemon #  The client and container runtime are now in separate packages from the daemon in Docker Engine 18.09. Users should install and update all three packages at the same time to get the latest patch releases.\nFor example, on Ubuntu:\nsudo apt install docker-ce docker-ce-cli containerd.io 18.06 (2018-07-18) #  17.03 (2017-03-01) #  Docker CE 和 Docker EE（17.03） #  新版本: Docker Engine release notes\n在 2017 年 3 月 2 日，docker 团队宣布企业版 Docker Enterprise Edition (EE) 发布。\n为了一致，免费的 Docker Engine 改名为 Docker Community Edition (CE), 并且采用基于时间的版本号方案。\n就在这一天，Docker EE 和 Docker CE 的 17.03 版本发布，这也是第一个采用新的版本号方案的版本。\nDocker CE/EE 每个季度发布一次 季度版本 , 也就是说每年会发布 4 个季度版本，17.03, 17.06, 17.09, 17.12 就是 2017 年的 4 个季度版本的版本号。\n同时 Docker CE 每个月还会发布一个 EDGE 版本，比如 17.04, 17.05, 17.07, 17.08, 17.10, 17.11。\nDocker CE 季度版本自发布后会有 4 个月的维护期。\n在基于时间的发布方案中，版本号格式为: YY.MM.，YY.MM 代表年月，patch 代表补丁号，从 0 开始，\n在季度版本 (如 17.03) 的维护期内，bug 修复相关的更新会以 patch 递增的方式发布，\n比如 17.03.0 -\u0026gt; 17.03.1 -\u0026gt; 17.03.2.\n1.13.1 之前 #  Docker Engine（0.1.0 到 1.13.1） #  Docker CE 在 1.13.1 及之前版本叫 Docker Engine 。\n版本说明参考：Docker Engine release notes (Previous versions)，\n可以看到 Docker Engine 的版本号范围: 0.1.0 (2013-03-23) ~ 1.13.1 (2017-02-08)\n 1.11 #  Docker 守护进程启动命令：\n 1.7: docker -d 1.8: docker daemon 1.11: dockerd  1.8 #  1.7 #  "}),a.add({id:33,href:'/study-kubernetes/docs/basic/other/version/',title:"版本",section:"1.4 其他",content:"Kubernetes 版本 #  kubernetes/kubernetes 查看版本 #  kuberctl #   kuberctl version  这里 Client Version 的 GitVersion 是 kuberctl 的版本：\napiserver #   kuberctl version  这里 Server Version 的 GitVersion 是 apiserver 的版本：\nkubelet #   kuberctl get nodes  这里的 VERSION 是 kubelet 的版本：\n 历史版本 #  v1.20 #  v1.19 #   v1.18 #  v1.18.6 #   v1.17 #   v1.16 #   v1.15 #   v1.14 (2019-03-26) #   v1.13 (2018-12-03) #   v1.12 (2018-09-27) #   v1.11 (2018-06-27) #   v1.10 (2018-03-26) #   v1.9 (2017-12-15) #   v1.8 (2017-09-28) #   v1.7 (2017-06-29) #   CRD (Custom Resource Definition)   v1.6 #  v1.5 #   CRI (Container Runtime Interface)   v1.0 #   参考 #  "}),a.add({id:34,href:'/study-kubernetes/docs/advanced/eco/docker/port/',title:"端口",section:"Docker",content:"Docker 端口 #  端口映射 #   -P（大写）：将容器内部开放的网络端口随机映射到宿主机的一个端口上 -p（小写）：指定要映射的端口，一个指定端口上只可以绑定一个容器  可以有多个 -p    端口映射格式 #  # 指定 ip、指定宿主机 port、指定容器 port ip:hostport:containerport # 指定 ip、未指定宿主机 port（随机）、指定容器 port ip::containerport # 未指定 ip、指定宿主机 port、指定容器 port hostport:containerport 端口映射命令 #  # 将容器暴露的所有端口，都随机映射到宿主机上（不推荐） docker run -P -it ubuntu /bin/bash # 将容器指定端口随机映射到宿主机一个随机端口 docker run -P 80 -it ubuntu /bin/bash # 将容器指定端口，随机映射到宿主机的指定 ip 的随机端口 # 有两个冒号 : docker run -P 192.168.0.100::80 -it ubuntu /bin/bash # 将容器指定端口指定映射到宿主机的指定端口上 docker run -p 8000:80 -it ubuntu /bin/bash # 将容器指定端口，指定映射到宿主机指定 ip 和端口 # 只能访问 192.168.0.100:8000， # 访问 127.0.0.1:8000 或 localhost:8000 都不行 docker run -p 192.168.0.100:8000:80 -it ubuntu /bin/bash 指定通信协议，比如 tcp、udp #  # tcp docker run -ti -d --name my-nginx5 -p 8099:80/tcp docker.io/nginx # udp docker run -ti -d --name my-nginx6 -p 192.168.10.214:8077:80/udp docker.io/nginx 查看映射端口配置 #  # 结果输出： 80/tcp -\u0026gt; 0.0.0.0:800 docker port container_ID # 容器 ID 通过宿主机的 iptables 进行 nat 转发 #  容器除了在启动时添加端口映射关系，还可以通过宿主机的 iptables 进行 nat 转发，将宿主机的端口映射到容器的内部端口上， 这种方式适用于容器启动时没有指定端口映射的情况。\n# 容器 my-nginx9 在启动时没有指定其内部的 80 端口映射到宿主机的端口上， # 所以默认是没法访问的 docker run -ti -d --name my-nginx9 docker.io/nginx # 首先获得容器的 ip 地址 docker inspect my-nginx9|grep IPAddress # 将容器的 80 端口映射到 dockers 宿主机的 9998 端口 iptables -t nat -A PREROUTING -p tcp -m tcp --dport 9998 -j DNAT --to-destination 172.17.0.9:80 iptables -t nat -A POSTROUTING -d 172.17.0.9/32 -p tcp -m tcp --sport 80 -j SNAT --to-source 192.16.10.214 iptables -t filter -A INPUT -p tcp -m state --state NEW -m tcp --dport 9998 -j ACCEPT # 保存以上 iptables 规则 iptables-save \u0026gt; /etc/sysconfig/iptables # 查看/etc/sysconfig/iptables 文件， # 注意下面两行有关 icmp-host-prohibited 的设置一定要注释掉，否则 nat 转发会失败 # -A INPUT -j REJECT --reject-with icmp-host-prohibited # -A FORWARD -j REJECT --reject-with icmp-host-prohibited # 最后重启 iptbales 服务 systemctl restart iptables # 查看 iptables 规则 iptables -L # 然后访问 http://192.168.10.214:9998/，就能转发访问到 my-nginx9 容器的 80 端口了  问答 #  启动 docker 容器时，有如下报错 #  /usr/bin/docker-current: Error response from daemon: driver failed programming external connectivity on endpoint my-nginx (db5a0edac68d1ea7ccaa3a1e0db31ebdf278076ef4851ee4250221af6167f9ac): (iptables failed: iptables --wait -t nat -A DOCKER -p tcp -d 0/0 --dport 8088 -j DNAT --to-destination 172.17.0.2:80 ! -i docker0: iptables: No chain/target/match by that name. 解决办法 1）不需要关闭防火墙 2）重启 docker 服务:systemctl restart docker 3）docker 服务重启后，所有容器都会关闭，应立即批量启动全部容器:docker start docker ps -a -q 启动的容器也会包括上面报错的容器，重启 docker 后，该容器就能正常启动和使用了。\nDocker 端口映射到宿主机后，外部无法访问对应宿主机端口 #  创建 docker 容器的时候,做了端口映射到宿主机, 防火墙已关闭, 但是外部始终无法访问宿主机端口? 这种情况基本就是因为宿主机没有开启 ip 转发功能，从而导致外部网络访问宿主机对应端口是没能转发到 Docker Container 所对应的端口上。\n解决办法: Linux 发行版默认情况下是不开启 ip 转发功能的。这是一个好的做法，因为大多数人是用不到 ip 转发的，但是如果架设一个 Linux 路由或者 VPN 服务我们就需要开启该服务了。\n在 Linux 中开启 ip 转发的内核参数为：net.ipv4.ip_forward，查看是否开启 ip 转发：\ncat /proc/sys/net/ipv4/ip_forward // 0：未开启，1：已开启 打开 ip 转发功能 #  临时打开 ip 转发功能!\n# echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forward # sysctl -w net.ipv4.ip_forward=1 永久生效的 ip 转发\nvim /etc/sysctl.conf net.ipv4.ip_forward = 1 # 立即生效 sysctl -p /etc/sysctl.conf Linux 系统中也可以通过重启网卡来立即生效。\n修改 sysctl.conf 文件后的生效\n# CentOS 6 service network restart # CentOS 7 systemctl restart network 参考：\n Docker 容器内部端口映射到外部宿主机端口 - 运维笔记  "}),a.add({id:35,href:'/study-kubernetes/docs/advanced/eco/docker/image/',title:"镜像",section:"Docker",content:"Docker 镜像 #  dangling 镜像 #  未被任何镜像引用的镜像\n docker push #  上传到 私有仓库 #  docker build -t human-attribute:20200415_1585818439123 . docker tag human-attribute:20200415_1585818439123 ote-harbor.baidu.com/aiedge/human-attribute:20200415_1585818439123 docker push ote-harbor.baidu.com/aiedge/human-attribute:20200415_1585818439123 "}),a.add({id:36,href:'/study-kubernetes/docs/advanced/eco/docker/containerd/',title:"containerd",section:"Docker",content:"containerd #  "}),a.add({id:37,href:'/study-kubernetes/docs/advanced/eco/docker/container/',title:"容器",section:"Docker",content:"Docker 容器 #  "}),a.add({id:38,href:'/study-kubernetes/docs/advanced/eco/docker/repository/',title:"仓库",section:"Docker",content:"Docker 仓库 #  "}),a.add({id:39,href:'/study-kubernetes/docs/advanced/eco/kata/',title:"Kata",section:"2.8 生态",content:"Kata #  Kata Containers is an open source project and community working to build a standard implementation of lightweight Virtual Machines (VMs) that feel and perform like containers, but provide the workload isolation and security advantages of VMs.\nhttps://katacontainers.io/\nv1 #  kata-containers/runtime v2 #  kata-containers/kata-containers  教程 #  "}),a.add({id:40,href:'/study-kubernetes/docs/advanced/eco/kubevirt/',title:"KubeVirt",section:"2.8 生态",content:"KubeVirt #  kubevirt/kubevirt Kubernetes Virtualization API and runtime in order to define and manage virtual machines.\nhttps://kubevirt.io/\n教程 #  "}),a.add({id:41,href:'/study-kubernetes/docs/basic/concept/',title:"1.2 概念",section:"第一部分 基础入门",content:"概念 #  "}),a.add({id:42,href:'/study-kubernetes/docs/advanced/tool/',title:"2.2 工具",section:"第二部分 进阶实战",content:"工具 #  "}),a.add({id:43,href:'/study-kubernetes/docs/code/crd/',title:"3.2 CRD",section:"第三部分 设计与实现",content:"CRD #  "}),a.add({id:44,href:'/study-kubernetes/docs/code/crd/operator/',title:"3.2.2 Operator",section:"3.2 CRD",content:"Operator #  "}),a.add({id:45,href:'/study-kubernetes/docs/appendix/interview/',title:"4.2 面试题",section:"第四部分 附录",content:"面试题 #  基础 #  进阶 #   其他 #  头条面试题 #  头条人选一（最近一周刚面试，面试岗位是头条杭州的 docker、k8s 工程师） 1.解释一下 acid 2.数据库的隔离级别 3.每个隔离级别的实现原理 4.讲一下分布式事务现有的方案，优缺点 5.说一下 cgroup 原理 6.说一下 mesos，k8s 的架构 7.说一下 actor 编程模型的原理和意义 算法：输出所有出现次数大于 n/k 的数,如果没有这样的数,请输出”-1“。 人选反馈：面试官会抓住一个点一直追问，问的比较细。 头条人选二（两周前面试 paas 平台开发工程师） 一面 1.字符串原地反转 2.软连接和硬连接的区别 3.前序和中序构造二叉树 4.10 亿个数字排序 5.大型企业的上线流程 二面 连续子数组的最大和 链表回文 三面 基于人选项目去发问，偏业务，没有算法题，还问到一些开源组件的使用情况。 头条人选三（面试基础架构部门容器岗位） 1. 系统 （Linux 下面进程是如何调度的，进程的优先级），有没有遇到过问题，描述原因和解决方法 2.网络 TIME_WAIT 状态出现的原因，排查的思路，有没有调优的方法 2. 数据库 MyISAM 和 InnoDB 的简单区别 3. Docker/K8s CGroup 如何做 cpu 的资源限制，哪几种限制方式 K8s 中的 request 和 limit 是如何实现的 K8s 中的 rc、rs、deployment 的区别，特点 5.算法 合并两个有序链表 头条人选四（基础架构部门研发） docker pull 镜像原理 registry 镜像存储原理 k8s deployment 创建过程 gpu 资源共享 scheduler 工作原理 docker 架构 docker exec 过程 goroutine 挂起 实现阻塞队列 项目 linux 进程调度 http 请求过程 tcp 建立连接和断开连接 goroutine 原理 flannel 工作原理 头条人选五（devops 偏容器的人选） 服务器负载是怎么计算的 如何查看服务执行卡住时服务器做了哪些事 查看进程打开哪些文件，查看哪些进程打开文件多 docker 网络模式有几种 分别是做什么 浏览器输入域名到返回结果过程 DNS 使用了什么协议，为什么要用这种协议 tcp 协议如何保证传输安全 timewait 状态的前后过程以及 timewait 过多时怎么处理 怎么优化 K8S 是怎么调度的 iptables 主要有哪些链和哪些表，分别是做什么用 docker 会调用 iptables 的哪些链 常驻空间和虚拟空间有什么区别 网络加速或者网络优化是怎么来做的，服务端和客户端分别可以做哪些 动态 CDN 和静态 CDN 的区别 算法题： 求两个文本文件的交集，并输出，比如 ABC 在 a.txt 有 5 行，在 b.txt 有 3 行 那么在结果文件中输出 3 行 "}),a.add({id:46,href:'/study-kubernetes/docs/basic/arch/network/dns/kubedns/',title:"KubeDNS",section:"K8s DNS",content:"KubeDNS #  在 Linux 系统中，/etc/resolv.conf 是存储 DNS 服务器的文件， 普通 Pod 的 /etc/resolv.conf 文件应该存储的是 kube-dns 的 Service IP。\nnameserver 10.99.0.2 # 这里存储的是 kube-dns 的 Service IP search default.svc.cluster.local. svc.cluster.local. cluster.local. options ndots:5 如何进入 kube-dns 容器进行抓包 #  DNS 容器往往不具备 bash，所以不能通过 docker exec 或者 kubectl exec 的方式进入容器抓包。\ndocker inspect --format \u0026quot;{{.State.Pid}}\u0026quot; dns_container_id # 进入 container 的 network namespace nsenter -n -t pid # 对 53 端口进行抓包 tcpdump -i eth0 -N udp dst port 53 dnsPolicy #  ClusterFirst（默认） #  优先使用 kubedns 或者 coredns 进行域名解析。 如果解析不成功，才会使用宿主机的 DNS 配置进行解析。\nClusterFirstWithHostNet #  当一个 Pod 以 HOST 模式（和宿主机共享网络，hostNetwork: true）启动时，这个 POD 中的所有容器都会使用宿主机的 /etc/resolv.conf 配置进行 DNS 查询。 但是如果在 Pod 中仍然还想继续使用 k8s 集群 的 DNS 服务时，就需要将 dnsPolicy 设置为 ClusterFirstWithHostNet。\nDefault #  让 kubelet 来决定 Pod 内的 DNS 使用哪种 DNS 策略。 kubelet 的默认方式，其实就是使用宿主机的 /etc/resolv.conf 来进行解析。 你可以通过设置 kubelet 的启动参数， --resolv-conf=/etc/resolv.conf 来决定该 DNS 服务使用的解析文件的地址\n当我们部署集群 DNS 服务的时候，一般就需要将 dnsPolicy 设置成 Default， 而并非使用默认值 ClusterFirst，否则该 DNS 服务的上游解析地址会变成它自身的 Service 的 ClusterIP（我解析我自己），导致域名无法解析。\nNone #  不会使用集群和宿主机的 DNS 策略，而是和 dnsConfig 配合一起使用，来自定义 DNS 配置，否则在提交修改时报错。\n kube-dns 组成 #  kubedns #  依赖 client-go 中的 informer 机制监视 k8s 中的 Service 和 Endpoint 的变化，并将这些结构维护进内存来服务内部 DNS 解析请求。\ndnsmasq #  区分 Domain 是集群内部还是外部，给外部域名提供上游解析，内部域名发往 10053 端口，并将解析结果缓存，提高解析效率。\nsidecar #  对 kubedns 和 dnsmasq 进行健康检查和收集监控指标。\n如何调试 DNS 解析 #  参考 k8smeetup：调试 DNS 解析\n"}),a.add({id:47,href:'/study-kubernetes/docs/advanced/tool/kustomize/',title:"Kustomize",section:"2.2 工具",content:"Kustomize #  kubernetes-sigs/kustomize Customization of kubernetes YAML configurations\nkustomize lets you customize raw, template-free YAML files for multiple purposes, leaving the original YAML untouched and usable as is.\n一般应用都会存在多套部署环境：开发环境、测试环境、生产环境，多套环境意味着存在多套 K8S 应用资源 YAML。而这么多套 YAML 之间只存在微小配置差异，比如镜像版本不同、Label 不同等，而这些不同环境下的 YAML 经常会因为人为疏忽导致配置错误。再者，多套环境的 YAML 维护通常是通过把一个环境下的 YAML 拷贝出来然后对差异的地方进行修改。一些类似 Helm 等应用管理工具需要额外学习 DSL 语法。总结以上，在 k8s 环境下存在多套环境的应用，经常遇到以下几个问题：\n 如何管理不同环境或不同团队的应用的 Kubernetes YAML 资源 如何以某种方式管理不同环境的微小差异，使得资源配置可以复用，减少 copy and change 的工作量 如何简化维护应用的流程，不需要额外学习模板语法  Kustomize 通过以下几种方式解决了上述问题：\n kustomize 通过 Base \u0026amp; Overlays 方式 (下文会说明) 方式维护不同环境的应用配置 kustomize 使用 patch 方式复用 Base 配置，并在 Overlay 描述与 Base 应用配置的差异部分来实现资源复用 kustomize 管理的都是 Kubernetes 原生 YAML 文件，不需要学习额外的 DSL 语法  "}),a.add({id:48,href:'/study-kubernetes/docs/basic/concept/resource/operator/',title:"Operator",section:"API 对象",content:"Operator #  Operator 的工作原理 #  利用了 Kubernetes 的自定义 API 资源（CRD），来描述我们想要部署的 “有状态应用”； 然后在自定义控制器里，根据自定义 API 对象的变化，来完成具体的部署和运维工作。\n所以，编写一个 Etcd Operator，与我们前面编写一个自定义控制器的过程，没什么不同。\n Operator 开发 #  kubernetes-sigs/kubebuilder #  kudobuilder/kudo #  helm 做安装，kudo 做 day2 运维\nGoogleCloudPlatform/metacontroller #  Lightweight Kubernetes controllers as a service https://metacontroller.app/\n可与 Webhook 结合使用，以实现自己的功能。\noperator-framework/operator-sdk #  SDK for building Kubernetes applications. Provides high level APIs, useful abstractions, and project scaffolding. https://sdk.operatorframework.io\nOperator 框架\noperator-framework/getting-started #   operator-framework/operator-lifecycle-manager #  A management framework for extending Kubernetes with Operators\n Operator 示例 #  Etcd Operator #  $ git clone https://github.com/coreos/etcd-operator # 因为，Etcd Operator 需要访问 Kubernetes 的 APIServer 来创建对象 # 为 Etcd Operator 创建 RBAC 规则 $ example/rbac/create_role.sh Etcd Operator 本身，其实就是一个 Deployment\n而一旦 Etcd Operator 的 Pod 进入了 Running 状态，你就会发现，有一个 CRD 被自动创建了出来\n这个 CRD 名叫 etcdclusters.etcd.database.coreos.com\n实际上是在 Kubernetes 里添加了一个名叫 EtcdCluster 的自定义资源类型（CRD）。\n而 Etcd Operator 本身，就是这个 CRD 对应的自定义控制器。\napiVersion: extensions/v1beta1 kind: Deployment metadata: name: etcd-operator spec: replicas: 1 template: metadata: labels: name: etcd-operator spec: containers: - name: etcd-operator image: quay.io/coreos/etcd-operator:v0.9.2 command: - etcd-operator env: - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name 当 Etcd Operator 部署好之后，接下来在这个 Kubernetes 里创建一个 Etcd 集群的工作就非常简单了。\n你只需要编写一个 EtcdCluster 的 YAML 文件，然后把它提交给 Kubernetes\n$ kubectl apply -f example/example-etcd-cluster.yaml example-etcd-cluster.yaml 定义的是 EtcdCluster 这个 CRD 的一个具体实例， 也就是一个 Custom Resource（CR）\nEtcdCluster 的 spec 字段非常简单。其中，size=3 指定了它所描述的 Etcd 集群的节点个数。 而 version=“3.2.13”，则指定了 Etcd 的版本，仅此而已。\n而真正把这样一个 Etcd 集群创建出来的逻辑，就是 Etcd Operator 要实现的主要工作\napiVersion: \u0026#34;etcd.database.coreos.com/v1beta2\u0026#34; kind: \u0026#34;EtcdCluster\u0026#34; metadata: name: \u0026#34;example-etcd-cluster\u0026#34; spec: size: 3 version: \u0026#34;3.2.13\u0026#34;  zookeeper operator #   参考 #   Introducing Operators: Putting Operational Knowledge into Software Best practices for building Kubernetes Operators and stateful apps  "}),a.add({id:49,href:'/study-kubernetes/docs/advanced/eco/docker/volume/',title:"数据卷",section:"Docker",content:"Docker 数据卷 #  "}),a.add({id:50,href:'/study-kubernetes/docs/basic/arch/architecture/',title:"架构",section:"1.3 架构",content:"Kubernetes 架构 #  "}),a.add({id:51,href:'/study-kubernetes/docs/basic/arch/',title:"1.3 架构",section:"第一部分 基础入门",content:"架构 #  "}),a.add({id:52,href:'/study-kubernetes/docs/advanced/feature/',title:"2.3 功能",section:"第二部分 进阶实战",content:"功能 #  存储 #  rook/rook #  Storage Orchestration for Kubernetes\n   Name Details API Group Status     Rook Framework The framework for common storage specs and logic used to support other storage providers. rook.io/v1alpha2 Alpha   Ceph Ceph is a distributed storage system that provides file, block and object storage and is deployed in large scale production clusters. ceph.rook.io/v1 Stable   CockroachDB CockroachDB is a cloud-native SQL database for building global, scalable cloud services that survive disasters. cockroachdb.rook.io/v1alpha1 Alpha   Cassandra Cassandra is a highly available NoSQL database featuring lightning fast performance, tunable consistency and massive scalability. Scylla is a close-to-the-hardware rewrite of Cassandra in C++, which enables much lower latencies and higher throughput. cassandra.rook.io/v1alpha1 Alpha   EdgeFS EdgeFS is high-performance and fault-tolerant decentralized data fabric with access to object, file, NoSQL and block. edgefs.rook.io/v1 Stable   NFS Network File System (NFS) allows remote hosts to mount file systems over a network and interact with those file systems as though they are mounted locally. nfs.rook.io/v1alpha1 Alpha   YugabyteDB YugabyteDB is a high-performance, cloud-native distributed SQL database which can tolerate disk, node, zone and region failures automatically. yugabytedb.rook.io/v1alpha1 Alpha    "}),a.add({id:53,href:'/study-kubernetes/docs/appendix/attention/',title:"4.3 关注项目",section:"第四部分 附录",content:"Kubernetes 关注项目 #  ramitsurana/awesome-kubernetes kubernetes 官方 #  kubernetes/kubernetes #  Production-Grade Container Scheduling and Management https://kubernetes.io\nkubernetes/client-go #  Go client for Kubernetes.\nkubernetes/kops #  kops - Kubernetes Operations The easiest way to get a production grade Kubernetes cluster up and running.\nkubernetes/dashboard #  General-purpose web UI for Kubernetes clusters\nkubernetes/kube-state-metrics #  Add-on agent to generate and expose cluster-level metrics.\nkubernetes/examples #  Kubernetes application example tutorials\nkubernetes/sample-controller #  Repository for sample controller. Complements sample-apiserver\nkubernetes/node-problem-detector #  This is a place for various problem detectors running on the Kubernetes nodes.\nkubernetes/enhancements #  Enhancements tracking repo for Kubernetes\nkubernetes/test-infra #  This repository contains tools and configuration files for the testing and automation needs of the Kubernetes project.\nkubernetes/kompose #  Go from Docker Compose to Kubernetes http://kompose.io\nkubernetes/ingress-nginx #  NGINX Ingress Controller for Kubernetes https://kubernetes.github.io/ingress-nginx/\n 安装 kubernetes #  kubernetes/minikube #  Run Kubernetes locally https://minikube.sigs.k8s.io\n一般用于本地开发、测试和学习，不能用于生产环境\nkubernetes/kubeadm #   用于快速搭建 kubernetes 集群，目前应该是比较方便和推荐的，简单易用 kubeadm 是 Kubernetes 1.4 开始新增的特性 kubeadm init 以及 kubeadm join 这两个命令可以快速创建 kubernetes 集群  ubuntu/microk8s #  MicroK8s is a small, fast, single-package Kubernetes for developers, IoT and edge. https://microk8s.io\n Kubernetes 周边工具 #  rancher/rancher #  Rancher 是一个容器管理平台，通过 Rancher 可以实现 Docker 和 Kubernetes 的轻松部署。\nhelm/helm #  The Kubernetes Package Manager https://helm.sh\nrancher/k3s #  Lightweight Kubernetes. 5 less than k8s. https://k3s.io\nkubernetes-sigs/kustomize #  Customization of kubernetes YAML configurations\n rancher/k3os #  Purpose-built OS for Kubernetes, fully managed by Kubernetes. https://k3os.io\nrancher/fleet #  Manage large fleets of Kubernetes clusters\nbaidu/ote-stack #  OTE-Stack is an edge computing platform for 5G and AI https://ote.baidu.com/\n其他 #  knative/serving #  Kubernetes-based, scale-to-zero, request-driven compute https://knative.dev/docs/serving\nkubeflow/kubeflow #  Machine Learning Toolkit for Kubernetes\nalauda/kube-ovn #  A Kubernetes Network Fabric for Enterprises that is Rich in Functions and Easy in Operations https://kube-ovn.io\nOpenNESS #  OpenNESS（Open Network Edge Services Software）是一个开源的边缘应用程序管理系统，使服务提供商和企业能够在任何网络的边缘上构建、部署和操作自己的边缘应用程序（ME APP），支持通过简易的方式将运行在 Telco/Public Cloud 中的 APP 迁移到边缘。\nOpenNESS, the easy button to deploy innovative services at the Edge. OpenNESS is an open source reference toolkit that makes it easy to move applications from the Cloud to the Network and On-Premise Edge.\nistio/istio #  Connect, secure, control, and observe services. https://istio.io\ndapr/dapr #  Dapr is a portable, event-driven, runtime for building distributed applications across cloud and edge.\n"}),a.add({id:54,href:'/study-kubernetes/docs/advanced/network/calico/',title:"Calico",section:"Kubernetes 网络",content:"Calico #  projectcalico/calico https://docs.projectcalico.org/\ncalico/kube-controllers #  "}),a.add({id:55,href:'/study-kubernetes/docs/advanced/feature/operator/etcd/',title:"etcd Operator",section:"Operator",content:"etcd Operator #  $ git clone https://github.com/coreos/etcd-operator # 因为，Etcd Operator 需要访问 Kubernetes 的 APIServer 来创建对象 # 为 Etcd Operator 创建 RBAC 规则 $ example/rbac/create_role.sh Etcd Operator 本身，其实就是一个 Deployment\n而一旦 Etcd Operator 的 Pod 进入了 Running 状态，你就会发现，有一个 CRD 被自动创建了出来\n这个 CRD 名叫 etcdclusters.etcd.database.coreos.com\n实际上是在 Kubernetes 里添加了一个名叫 EtcdCluster 的自定义资源类型（CRD）。\n而 Etcd Operator 本身，就是这个 CRD 对应的自定义控制器。\napiVersion: extensions/v1beta1 kind: Deployment metadata: name: etcd-operator spec: replicas: 1 template: metadata: labels: name: etcd-operator spec: containers: - name: etcd-operator image: quay.io/coreos/etcd-operator:v0.9.2 command: - etcd-operator env: - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name 当 Etcd Operator 部署好之后，接下来在这个 Kubernetes 里创建一个 Etcd 集群的工作就非常简单了。\n你只需要编写一个 EtcdCluster 的 YAML 文件，然后把它提交给 Kubernetes\n$ kubectl apply -f example/example-etcd-cluster.yaml example-etcd-cluster.yaml 定义的是 EtcdCluster 这个 CRD 的一个具体实例， 也就是一个 Custom Resource（CR）\nEtcdCluster 的 spec 字段非常简单。其中，size=3 指定了它所描述的 Etcd 集群的节点个数。 而 version=“3.2.13”，则指定了 Etcd 的版本，仅此而已。\n而真正把这样一个 Etcd 集群创建出来的逻辑，就是 Etcd Operator 要实现的主要工作\napiVersion: \u0026#34;etcd.database.coreos.com/v1beta2\u0026#34; kind: \u0026#34;EtcdCluster\u0026#34; metadata: name: \u0026#34;example-etcd-cluster\u0026#34; spec: size: 3 version: \u0026#34;3.2.13\u0026#34; "}),a.add({id:56,href:'/study-kubernetes/docs/advanced/tool/helm/',title:"Helm",section:"2.2 工具",content:"HELM #  helm/helm  Helm is the best way to find, share, and use software built for Kubernetes.\n 官网：https://helm.sh\nHelm is a tool for managing Charts. Charts are packages of pre-configured Kubernetes resources.\nUse Helm to:\n Find and use popular software packaged as Helm Charts to run in Kubernetes Share your own applications as Helm Charts Create reproducible builds of your Kubernetes applications Intelligently manage your Kubernetes manifest files Manage releases of Helm packages   基本概念 #   Chart：一个 Helm 包，其中包含了运行一个应用所需要的镜像、依赖和资源定义等，还可能包含 Kubernetes 集群中的服务定义，  Chart 是用来封装 Kubernetes 原生应用程序的一系列 YAML 文件集合 类似  Homebrew 中的 formula apt 的 dpkg yum/dnf 的 rpm     Repository：用于发布和存储 Chart 的存储库。 Release：在 Kubernetes 集群上运行的 Chart 的一个实例。在同一个集群上，一个 Chart 可以安装很多次。每次安装都会创建一个新的 release。例如一个 MySQL Chart，如果想在服务器上运行两个数据库，就可以把这个 Chart 安装两次。每次安装都会生成自己的 Release，会有自己的 Release 名称。 Tiller 是 Helm2 的服务端，通常运行在您的 kubernetes 集群中。Tiller 用于接收 Helm 的请求，并根据 Chart 生成 Kubernetes 的部署文件，然后提交给 Kubernetes 创建应用。  HElm3 中删除了 Tiller，只有客户端使用，调用 ~/.kube/config 来访问 api server 进行创建    命令 #  completion generate autocompletions script for the specified shell create create a new chart with the given name dependency manage a chart\u0026#39;s dependencies env helm client environment information get download extended information of a named release help Help about any command history fetch release history install install a chart lint examine a chart for possible issues list list releases package package a chart directory into a chart archive plugin install, list, or uninstall Helm plugins pull download a chart from a repository and (optionally) unpack it in local directory repo add, list, remove, update, and index chart repositories rollback roll back a release to a previous revision search search for a keyword in charts show show information of a chart status display the status of the named release template locally render templates test run tests for a release uninstall uninstall a release upgrade upgrade a release verify verify that a chart at the given path has been signed and is valid version print the client version information  开源镜像 #  BurdenBear/kube-charts-mirror #  # 删除默认的源 helm repo remove stable helm repo remove incubator # 增加新的国内镜像源 helm repo add stable http://mirror.azure.cn/kubernetes/charts/ helm repo add incubator http://mirror.azure.cn/kubernetes/charts-incubator/ helm repo list helm search repo mysql  TODO #  # Initialize a Helm Chart Repository $ helm repo add stable https://kubernetes-charts.storage.googleapis.com/ # list the charts you can install $ helm search repo stable # Install an Example Chart $ helm repo update $ helm install stable/mysql --generate-name NAME: mysql-1596179301 LAST DEPLOYED: Fri Jul 31 15:08:27 2020 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: MySQL can be accessed via port 3306 on the following DNS name from within your cluster: mysql-1596179301.default.svc.cluster.local To get your root password run: MYSQL_ROOT_PASSWORD=$(kubectl get secret --namespace default mysql-1596179301 -o jsonpath=\u0026#34;{.data.mysql-root-password}\u0026#34; | base64 --decode; echo) To connect to your database: 1. Run an Ubuntu pod that you can use as a client: kubectl run -i --tty ubuntu --image=ubuntu:16.04 --restart=Never -- bash -il 2. Install the mysql client: $ apt-get update \u0026amp;\u0026amp; apt-get install mysql-client -y 3. Connect using the mysql cli, then provide your password: $ mysql -h mysql-1596179301 -p To connect to your database directly from outside the K8s cluster: MYSQL_HOST=127.0.0.1 MYSQL_PORT=3306 # Execute the following command to route the connection: kubectl port-forward svc/mysql-1596179301 3306 mysql -h ${MYSQL_HOST} -P${MYSQL_PORT} -u root -p${MYSQL_ROOT_PASSWORD} "}),a.add({id:57,href:'/study-kubernetes/docs/advanced/eco/k3s/',title:"K3s",section:"2.8 生态",content:"K3s #  rancher/k3s 教程 #  "}),a.add({id:58,href:'/study-kubernetes/docs/advanced/eco/oam/kubevela/',title:"KubeVela",section:"OAM",content:"KubeVela #  oam-dev/kubevela Make shipping applications more enjoyable. https://kubevela.io/\nps: oam-dev/rudr 已经被废弃，转为 KubeVela\n"}),a.add({id:59,href:'/study-kubernetes/docs/advanced/eco/oam/',title:"OAM",section:"2.8 生态",content:"OAM (Open Application Model) #  OAM 是阿里巴巴和微软共同开源的云原生应用规范模型\nRudr 的应用程序有三个元素：Components（组件）、Configuration（配置）、Traits（特征）：\n组件定义一个或多个面向操作系统的容器镜像以及硬件需求，如 CPU、内存和存储等 配置处理运行时的参数，比如环境变量 特征声明运行时的属性，比如 Volume、Ingress、伸缩等等。\n2019 年 10 月宣布开源，同时开源了基于 OAM 的实现 Rudr。\noam-dev/spec #  The Open Application Model specification\noam-dev/rudr #  A Kubernetes implementation of the Open Application Model specification https://oam.dev\nRudr 已经被废弃，转为 KubeVela\noam-dev/kubevela #  Make shipping applications more enjoyable. https://kubevela.io/\nOpenTelemetry #  Specifications for OpenTelemetry https://opentelemetry.io\n参考：\n OAM（开放应用模型）—— 定义云原生应用标准的野望  "}),a.add({id:60,href:'/study-kubernetes/docs/advanced/feature/operator/',title:"Operator",section:"2.3 功能",content:"Operator #  Operator 的工作原理 #  利用了 Kubernetes 的自定义 API 资源（CRD），来描述我们想要部署的 “有状态应用”； 然后在自定义控制器里，根据自定义 API 对象的变化，来完成具体的部署和运维工作。\n所以，编写一个 Etcd Operator，与我们前面编写一个自定义控制器的过程，没什么不同。\n Operator 开发 #  Kubebuilder #  kubernetes-sigs/kubebuilder https://kubebuilder.io\nkudobuilder/kudo #  GoogleCloudPlatform/metacontroller #  Lightweight Kubernetes controllers as a service https://metacontroller.app/\n可与 Webhook 结合使用，以实现自己的功能。\noperator-framework/operator-sdk #  SDK for building Kubernetes applications. Provides high level APIs, useful abstractions, and project scaffolding.\nhttps://sdk.operatorframework.io\nOperator 框架\noperator-framework/getting-started #   operator-framework/operator-lifecycle-manager #  A management framework for extending Kubernetes with Operators\n  Operator 示例 #  operator-framework/awesome-operators #  Etcd Operator #  coreos/etcd-operator  zookeeper operator #   参考 #    Introducing Operators: Putting Operational Knowledge into Software\n  Best practices for building Kubernetes Operators and stateful apps\n  如何看待 kubebuilder 与 Operator Framework (Operator SDK) ？\n  Integrating Kubebuilder and Operator SDK\n  "}),a.add({id:61,href:'/study-kubernetes/docs/code/crd/operator/operatorhub/',title:"OperatorHub",section:"3.2.2 Operator",content:"OperatorHub #  https://operatorhub.io/\n"}),a.add({id:62,href:'/study-kubernetes/docs/basic/concept/resource/volume/',title:"Volume",section:"API 对象",content:"Volume #  Volume 必须和 pod 在同一个 namespace #  All sources are required to be in the same namespace as the Pod.\n参考：\n kubernetes.io: volumes all-in-one volume design document  "}),a.add({id:63,href:'/study-kubernetes/docs/basic/other/distro/',title:"发行版",section:"1.4 其他",content:"Kubernetes 发行版 #   Rancher 偏运维  RKE Rancher Kubernetes Engine rancher/rke    OpenShift 偏开发 AWS EKS Elastic Kubernetes Service Google GKE Google Kubernetes Engine Microsoft AKS Azure Kubernetes Service CDK Canonical Distribution of Kubernetes （Canonical 是 Ubuntu Linux 的制造商） CoreOS Tectonic/Red Hat CoreOS Docker 社区版 / Docker 企业版 Heptio Kubernetes 订阅版（Kubernetes 的两位创始人 Craig McLuckie 和 Joe Beda，创办了 Heptio） 2018 年 VMware 收购了 Heptio，不过此次收购目前暂未影响 Heptio 的产品计划 Kontena Pharos PKS Pivotal Container Service SUSE 容器服务平台 Telekube kubesphere/kubesphere KubeSphere 是在 Kubernetes 之上构建的以应用为中心的多租户容器平台，提供全栈的 IT 自动化运维的能力，简化企业的 DevOps 工作流。 KubeOperator/KubeOperator KubeOperator 是一个开源的轻量级 Kubernetes 发行版，专注于帮助企业规划、部署和运营生产级别的 K8s 集群。  参考：\n 10 Kubernetes distributions leading the container revolution 5 Reasons Not to Use Kubernetes Distributions Kubesphere 与 Rancher 有什么区别？ 部署生产级别的 Kubernetes 集群要注意哪些问题？ 如何在 CentOS7 上部署 Kubernetes 集群？  "}),a.add({id:64,href:'/study-kubernetes/docs/basic/arch/component/',title:"组件",section:"1.3 架构",content:"Kubernetes 组件 #  "}),a.add({id:65,href:'/study-kubernetes/docs/basic/other/',title:"1.4 其他",section:"第一部分 基础入门",content:"其他 #  "}),a.add({id:66,href:'/study-kubernetes/docs/advanced/eco/docker/network/',title:"Docker 网络",section:"Docker",content:"Docker 网络 #  网络模式 #   bridge 模式  使用 –net=bridge 指定，默认   host 模式  使用 –net=host 指定 一些对安全性有求高并且不需要联网的应用可以使用 none 网络   none 模式  使用 –net=none 指定   container 模式  使用 –net=container:NAMEorID 指定    bridge 模式 #  Docker Daemon 启动时默认会创建 Docker0 这个网桥，网段为 172.17.0.0/16,\n宿主机 IP 为 172.17.0.1 , 作为这个虚拟子网的网关。\n创建网桥 #  新建一个名为 anyesu_net 网段为 172.18.0.0/16 的网桥：\ndocker network create --subnet=172.18.0.0/16 anyesu_net 启动容器时指定 --net anyesu_net 即可。\nhost 模式 #  优势 #  host 网络最大的好处就是性能\n劣势 #  host 网络不便之处就是考虑端口冲突问题\n参考：\n Docker 下的网络模式  "}),a.add({id:67,href:'/study-kubernetes/docs/advanced/eco/kubeedge/',title:"KubeEdge",section:"2.8 生态",content:"KubeEdge #  kubeedge/kubeedge 教程 #  "}),a.add({id:68,href:'/study-kubernetes/docs/advanced/debug/',title:"2.5 故障排查",section:"第二部分 进阶实战",content:"故障排查 #  server localhost:8080 was refused #  $ k get node The connection to the server localhost:8080 was refused - did you specify the right host or port?  参考 #   监控、日志和排错  "}),a.add({id:69,href:'/study-kubernetes/docs/advanced/eco/knative/',title:"Knative",section:"2.8 生态",content:"Knative #  knative/serving Kubernetes-based, scale-to-zero, request-driven compute\nhttps://knative.dev/docs/serving\n Kubernetes 是容器平台，负责运行和管理容器 Knative 是代码平台，负责容器的构建、运行、扩展和路由   教程 #  Getting Started with Knative - Building Modern Serverless Workloads on Kubernetes #  Getting Started with Knative 是一本由 Pivotal 公司赞助 O’Reilly 出品的免费电子书\n社区翻译版本：Knative 入门 —— 构建基于 Kubernetes 的现代化 Serverless 应用\n "}),a.add({id:70,href:'/study-kubernetes/docs/advanced/eco/kubeflow/',title:"Kubeflow",section:"2.8 生态",content:"Kubeflow #  kubeflow/kubeflow Machine Learning Toolkit for Kubernetes\n教程 #  "}),a.add({id:71,href:'/study-kubernetes/docs/advanced/prof/',title:"2.6 性能",section:"第二部分 进阶实战",content:"性能 #  "}),a.add({id:72,href:'/study-kubernetes/docs/advanced/eco/kubeless/',title:"Kubeless",section:"2.8 生态",content:"Kubeless #  kubeless/kubeless Kubernetes Native Serverless Framework\n"}),a.add({id:73,href:'/study-kubernetes/docs/advanced/test/test/',title:"2.7 测试",section:"第二部分 进阶实战",content:"测试 #  open-policy-agent/conftest #  Write tests against structured configuration data using the Open Policy Agent Rego query language\n"}),a.add({id:74,href:'/study-kubernetes/docs/advanced/tool/client-go/',title:"client-go",section:"2.2 工具",content:"kubernetes/client-go #  安装 #  版本 #  v0.x.y #  Kubernetes releases \u0026gt;= v1.17.0\nkubernetes-1.x.y #  Kubernetes releases \u0026lt; v1.17.0\n"}),a.add({id:75,href:'/study-kubernetes/docs/code/source/1.19.3/',title:"1.19.3",section:"3.7 源码分析",content:"Kubernetes 1.19.3 源码分析 #  https://github.com/kubernetes/kubernetes/tree/v1.19.3\n 代码行数 #  152 万行\nXAMPPRocky/tokei $ tokei -V tokei 12.0.4 compiled with serialization support: json, cbor, yaml $ tokei --sort code --exclude vendor --exclude test =============================================================================== Language Files Lines Code Comments Blanks =============================================================================== Go 8046 1954427 1522897 261453 170077 JSON 438 455020 455020 0 0 YAML 745 104781 103873 677 231 Shell 219 31646 20558 7221 3867 Protocol Buffers 78 25224 7269 13770 4185 PowerShell 5 3212 2261 702 249 Makefile 15 1394 684 524 186 Python 5 650 435 103 112 Autoconf 2 464 428 32 4 SVG 4 386 378 4 4 Dockerfile 14 396 119 213 64 C 2 104 60 32 12 BASH 4 14 2 8 4 Plain Text 18 271 0 225 46 ------------------------------------------------------------------------------- Markdown 197 13193 0 9544 3649 |- BASH 26 415 382 21 12 |- Go 3 24 15 5 4 |- JSON 2 118 118 0 0 |- Shell 2 198 152 13 33 |- YAML 24 2263 1823 417 23 (Total) 16211 2490 10000 3721 =============================================================================== Total 9792 2594200 2116474 294964 182762 =============================================================================== "}),a.add({id:76,href:'/study-kubernetes/docs/advanced/eco/',title:"2.8 生态",section:"第二部分 进阶实战",content:"生态 #  "}),a.add({id:77,href:'/study-kubernetes/docs/advanced/feature/hpa/',title:"Horizontal Pod Autoscaler",section:"2.3 功能",content:"Horizontal Pod Autoscaler #  Pod 水平自动伸缩\n"}),a.add({id:78,href:'/study-kubernetes/docs/code/source/1.0.0/',title:"1.0.0",section:"3.7 源码分析",content:"Kubernetes 1.0.0 源码分析 #  https://github.com/kubernetes/kubernetes/tree/v1.0.0\n 代码行数 #  61 万行\nXAMPPRocky/tokei $ tokei -V tokei 12.0.4 compiled with serialization support: json, cbor, yaml $ tokei --sort code --exclude test =============================================================================== Language Files Lines Code Comments Blanks =============================================================================== Go 3765 803073 611213 104189 87671 JavaScript 411 153108 88958 50674 13476 JSON 154 44424 44420 0 4 CSS 173 26388 22087 1156 3145 Shell 208 23742 14575 5960 3207 SVG 58 12895 12847 41 7 YAML 195 6751 5180 1132 439 Protocol Buffers 13 3581 2220 786 575 BASH 21 2006 1475 245 286 Python 23 1886 1178 405 303 LESS 7 1132 912 54 166 Makefile 50 1092 559 285 248 Dockerfile 66 1051 491 369 191 C 2 215 175 13 27 XML 4 140 140 0 0 Java 1 154 126 10 18 Sass 3 153 124 0 29 Autoconf 2 111 109 2 0 CoffeeScript 1 123 108 3 12 Assembly 1 57 36 10 11 PHP 1 33 27 0 6 INI 2 10 9 0 1 TOML 1 3 3 0 0 Plain Text 44 5486 0 4626 860 ------------------------------------------------------------------------------- HTML 96 6606 5616 67 923 |- CSS 12 20 20 0 0 |- JavaScript 16 158 122 15 21 (Total) 6784 5758 82 944 ------------------------------------------------------------------------------- Markdown 396 35167 0 25424 9743 |- BASH 27 620 573 24 23 |- C 1 46 29 15 2 |- Go 42 1941 1381 285 275 |- HTML 7 99 67 8 24 |- INI 1 10 6 2 2 |- JavaScript 5 52 52 0 0 |- JSON 13 741 729 0 12 |- PHP 1 33 27 0 6 |- Python 1 12 7 1 4 |- Shell 36 828 791 7 30 |- YAML 23 1064 1024 40 0 (Total) 40613 4686 25806 10121 =============================================================================== Total 5698 1135011 817416 195848 121747 =============================================================================== "}),a.add({id:79,href:'/study-kubernetes/docs/code/source/',title:"3.7 源码分析",section:"第三部分 设计与实现",content:"源码分析 #  Kubernetes 要求的 Go 版本 #     Kubernetes requires Go     1.0 - 1.2 1.4.2   1.3, 1.4 1.6   1.5, 1.6 1.7 - 1.7.5   1.7 1.8.1   1.8 1.8.3   1.9 1.9.1   1.10 1.9.1   1.11 1.10.2   1.12 1.10.4   1.13 1.11.13   1.14 - 1.16 1.12.9   1.17 - 1.18 1.13.15   1.18+ 1.15    参考：\n Kubernetes Development Guide  "}),a.add({id:80,href:'/study-kubernetes/docs/advanced/network/',title:"Kubernetes 网络",section:"第二部分 进阶实战",content:"Kubernetes 网络 #  "}),a.add({id:81,href:'/study-kubernetes/docs/advanced/eco/docker/tutorial/',title:"教程",section:"Docker",content:"Docker 教程 #  p8952/bocker #  Docker implemented in around 100 lines of bash\nyeasy/docker_practice #  Learn and understand Docker technologies, with real DevOps practice!\nDocker 从入门到实践\nplay-with-docker/play-with-docker #  Play With Docker gives you the experience of having a free Alpine Linux Virtual Machine in the cloud where you can build and run Docker containers and even create clusters with Docker features like Swarm Mode.\nUnder the hood DIND or Docker-in-Docker is used to give the effect of multiple VMs/PCs.\nA live version is available at: http://play-with-docker.com/\n"}),a.add({id:82,href:'/study-kubernetes/docs/advanced/eco/docker/attention/',title:"4.3 关注项目",section:"Docker",content:"关注 #  moby/moby #  Moby Project - a collaborative project for the container ecosystem to assemble container-based systems https://mobyproject.org/\ndocker/docker-ce #  Docker CE https://www.docker.com/community-edition\nplay-with-docker/play-with-docker #  Play With Docker gives you the experience of having a free Alpine Linux Virtual Machine in the cloud where you can build and run Docker containers and even create clusters with Docker features like Swarm Mode.\nUnder the hood DIND or Docker-in-Docker is used to give the effect of multiple VMs/PCs.\nA live version is available at: http://play-with-docker.com/\ngliderlabs/logspout #  Log routing for Docker container logs\n"}),a.add({id:83,href:'/study-kubernetes/docs/advanced/eco/docker/build/',title:"build",section:"Docker",content:"build #  "})})()